{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirmal/tau_dl_proj/blob/master/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDfsYl16STzg",
        "colab_type": "code",
        "outputId": "7f36cf15-297f-495f-b9fb-e21e86fc4ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/Deep learning/HW2/PTB.zip\" ./\n",
        "!mkdir -p ./PTB/\n",
        "!unzip -o ./PTB.zip -d ./PTB/\n",
        "cuda = torch.device('cuda')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  ./PTB.zip\n",
            "  inflating: ./PTB/ptb.char.train.txt  \n",
            "  inflating: ./PTB/ptb.char.valid.txt  \n",
            "  inflating: ./PTB/ptb.test.txt      \n",
            "  inflating: ./PTB/ptb.train.txt     \n",
            "  inflating: ./PTB/ptb.valid.txt     \n",
            "  inflating: ./PTB/README            \n",
            "  inflating: ./PTB/ptb.char.test.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QOApypzGiay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ARGS = {}\n",
        "ARGS['BATCH_SIZE'] = 20\n",
        "ARGS['EPOCHS'] = 13\n",
        "ARGS['BPTT_LEN'] = 20         # sequence length to unroll / backpropegate through time\n",
        "ARGS['HIDDEN_DIM'] = 200      # hidden state vector dimension\n",
        "ARGS['N_LAYERS'] = 2          # Number of hidden layers\n",
        "ARGS['LOG_BATCH_INTVL'] = 600 # print training info every <VAL> minibatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1RwjUuKVAJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.datasets import LanguageModelingDataset\n",
        " \n",
        "TEXT = data.Field(lower=True, tokenize='spacy', unk_token='<unk>')\n",
        "\n",
        "ptb_train = LanguageModelingDataset(\"./PTB/ptb.train.txt\", TEXT)\n",
        "ptb_valid = LanguageModelingDataset(\"./PTB/ptb.valid.txt\", TEXT)\n",
        "#ptb_test = LanguageModelingDataset(\"./PTB/ptb.test.txt\", TEXT)\n",
        "\n",
        "TEXT.build_vocab(ptb_train)\n",
        "\n",
        "train_iter = data.BPTTIterator(\n",
        "    ptb_train,\n",
        "    batch_size=ARGS['BATCH_SIZE'],\n",
        "    bptt_len=ARGS['BPTT_LEN'],\n",
        "    device=cuda,\n",
        "    repeat=False,\n",
        "    shuffle=True)\n",
        "valid_iter = data.BPTTIterator(\n",
        "    ptb_valid,\n",
        "    batch_size=ARGS['BATCH_SIZE'],\n",
        "    bptt_len=ARGS['BPTT_LEN'],\n",
        "    device=cuda,\n",
        "    repeat=False,\n",
        "    shuffle=True)\n",
        "\n",
        "#################################\n",
        "## unit test\n",
        "#################################\n",
        "run = False\n",
        "\n",
        "if run is True:\n",
        "  print(len(train_iter))\n",
        "\n",
        "  for batch in train_iter:\n",
        "    print(batch)\n",
        "    text, target = batch.text, batch.target\n",
        "    text, target = text[:,0], target[:,0]\n",
        "    print(text)\n",
        "    print(len(text))\n",
        "    print(target)\n",
        "    print(len(target))\n",
        "\n",
        "    text_s = [TEXT.vocab.itos[word_idx] for word_idx in text]\n",
        "    target_s = [TEXT.vocab.itos[word_idx] for word_idx in target]\n",
        "\n",
        "    print(text_s)\n",
        "    print(target_s)\n",
        "\n",
        "    break\n",
        "###################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfaXQHAl7UFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLSTM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyLSTM, self).__init__()\n",
        "    self.lstm = torch.nn.LSTM(ARGS['HIDDEN_DIM'], ARGS['HIDDEN_DIM'], ARGS['N_LAYERS'])\n",
        "\n",
        "    # we need a encoder/decoder to convert to/from one-hot vectors of the vocabulary\n",
        "    self.encoder = nn.Embedding(len(TEXT.vocab), ARGS['HIDDEN_DIM'])\n",
        "    self.encoder.weight.data.uniform_(-0.1, 0.1)\n",
        "    self.decoder = nn.Linear(ARGS['HIDDEN_DIM'], len(TEXT.vocab))\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    self.num_layers = self.lstm.num_layers\n",
        "    # End\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    # https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "    # we can do the entire sequence all at once.\n",
        "    # the first value returned by LSTM is all of the hidden states throughout\n",
        "    # the sequence. the second is just the most recent hidden state\n",
        "    # (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "    # The reason for this is that:\n",
        "    # \"out\" will give you access to all hidden states in the sequence\n",
        "    # \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "    # by passing it as an argument  to the lstm at a later time\n",
        "    # Add the extra 2nd dimension\n",
        "\n",
        "    x = self.encoder(input)\n",
        "    output, hidden = self.lstm(x, hidden)\n",
        "    decoded = self.decoder(output)\n",
        "    return decoded, hidden\n",
        "    # End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XHKDSkEIR6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspired by : https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    # Wraps hidden states in new Tensors, to detach them from their history.\n",
        "    #\n",
        "    # detach h,c so we can safely use them as input to next minibatch\n",
        "    # this make sure that gradients dont backprop between minibatches\n",
        "    # https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "# End\n",
        "\n",
        "\n",
        "def train_epoch(model, e, criter, opt):\n",
        "  # loss and time variables for logging prints (not used in model training)\n",
        "  total_loss = 0.\n",
        "  start_time = time.time()\n",
        "  model.to(cuda)\n",
        "  model.train() \n",
        "  h = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "  c = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "  hidden = (h, c)\n",
        "  for i, batch in enumerate(train_iter):\n",
        "    data, target = batch.text, batch.target\n",
        "    data.to(cuda), target.to(cuda)\n",
        "    model.zero_grad()\n",
        "    for v in hidden: v.to(cuda)\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    output, hidden = model(data, hidden)\n",
        "    loss = criter(output.view(-1, output.shape[-1]), target.view(-1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    total_loss += loss.item()\n",
        "    # log info\n",
        "    if i % ARGS['LOG_BATCH_INTVL'] == 0 and i > 0:\n",
        "      cur_loss = total_loss / ARGS['LOG_BATCH_INTVL']\n",
        "      elapsed = time.time() - start_time\n",
        "      print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "          e, i, len(train_iter), elapsed * 1000 / ARGS['LOG_BATCH_INTVL'], cur_loss, math.exp(cur_loss)))\n",
        "      total_loss = 0\n",
        "      start_time = time.time()\n",
        "# End\n",
        "\n",
        "def lr_scheduler_factor(e):\n",
        "# \"We train it for 4 epochs with a learning rate of 1 and then we decrease the learning rate\n",
        "# by a factor of 2 after each epoch, for a total of 13 training epochs\"\n",
        "  if e < 4:\n",
        "    return 1\n",
        "  elif e > 13:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0.5\n",
        "# End\n",
        "\n",
        "def test(model, test_iter, criter):\n",
        "    model.to(cuda)\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        h = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "        c = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "        hidden = (h, c)\n",
        "        for i, batch in enumerate(test_iter):\n",
        "            data, target = batch.text, batch.target\n",
        "            data.to(cuda), target.to(cuda)\n",
        "            for v in hidden: v.to(cuda)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            loss = criter(output.view(-1, output.shape[-1]), target.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(test_iter)\n",
        "\n",
        "def train(model):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "  lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_scheduler_factor)\n",
        "  for e in range(ARGS['EPOCHS']):\n",
        "    epoch_start_time = time.time()\n",
        "    train_epoch(model, e, criterion, optimizer)\n",
        "    val_loss = test(model, valid_iter, criterion)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | lr {} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        e, (time.time() - epoch_start_time), lr_scheduler.get_lr(), val_loss, math.exp(val_loss)))\n",
        "    lr_scheduler.step() # update the LR for next epoch\n",
        "# End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGJfD5rDChe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a5eeab6-a1ae-4934-c65e-dfeb288e06c4"
      },
      "source": [
        "lstm = MyLSTM()\n",
        "#test(lstm, valid_iter)\n",
        "train(lstm)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   0 |   600/ 2713 batches | ms/batch  4.09 | loss  6.39 | ppl   595.05\n",
            "| epoch   0 |  1200/ 2713 batches | ms/batch  3.83 | loss  5.65 | ppl   284.79\n",
            "| epoch   0 |  1800/ 2713 batches | ms/batch  3.80 | loss  5.41 | ppl   223.68\n",
            "| epoch   0 |  2400/ 2713 batches | ms/batch  3.81 | loss  5.28 | ppl   197.07\n",
            "| end of epoch   0 | time: 10.82s | lr [1] | valid loss  5.26 | valid ppl   193.21\n",
            "| epoch   1 |   600/ 2713 batches | ms/batch  4.08 | loss  5.21 | ppl   183.54\n",
            "| epoch   1 |  1200/ 2713 batches | ms/batch  3.83 | loss  5.11 | ppl   166.41\n",
            "| epoch   1 |  1800/ 2713 batches | ms/batch  3.83 | loss  4.99 | ppl   147.63\n",
            "| epoch   1 |  2400/ 2713 batches | ms/batch  3.80 | loss  4.93 | ppl   137.99\n",
            "| end of epoch   1 | time: 10.81s | lr [1] | valid loss  4.93 | valid ppl   138.95\n",
            "| epoch   2 |   600/ 2713 batches | ms/batch  4.07 | loss  4.88 | ppl   132.10\n",
            "| epoch   2 |  1200/ 2713 batches | ms/batch  3.80 | loss  4.82 | ppl   123.66\n",
            "| epoch   2 |  1800/ 2713 batches | ms/batch  3.81 | loss  4.73 | ppl   113.74\n",
            "| epoch   2 |  2400/ 2713 batches | ms/batch  3.83 | loss  4.70 | ppl   109.57\n",
            "| end of epoch   2 | time: 10.82s | lr [1] | valid loss  4.77 | valid ppl   118.43\n",
            "| epoch   3 |   600/ 2713 batches | ms/batch  4.09 | loss  4.69 | ppl   108.44\n",
            "| epoch   3 |  1200/ 2713 batches | ms/batch  3.81 | loss  4.64 | ppl   103.23\n",
            "| epoch   3 |  1800/ 2713 batches | ms/batch  3.81 | loss  4.57 | ppl    96.82\n",
            "| epoch   3 |  2400/ 2713 batches | ms/batch  3.82 | loss  4.55 | ppl    94.19\n",
            "| end of epoch   3 | time: 10.81s | lr [1] | valid loss  4.66 | valid ppl   105.41\n",
            "| epoch   4 |   600/ 2713 batches | ms/batch  4.09 | loss  4.54 | ppl    93.95\n",
            "| epoch   4 |  1200/ 2713 batches | ms/batch  3.81 | loss  4.50 | ppl    89.70\n",
            "| epoch   4 |  1800/ 2713 batches | ms/batch  3.80 | loss  4.44 | ppl    84.93\n",
            "| epoch   4 |  2400/ 2713 batches | ms/batch  3.81 | loss  4.42 | ppl    83.43\n",
            "| end of epoch   4 | time: 10.80s | lr [0.5] | valid loss  4.57 | valid ppl    96.39\n",
            "| epoch   5 |   600/ 2713 batches | ms/batch  4.05 | loss  4.47 | ppl    87.38\n",
            "| epoch   5 |  1200/ 2713 batches | ms/batch  3.80 | loss  4.43 | ppl    83.83\n",
            "| epoch   5 |  1800/ 2713 batches | ms/batch  3.80 | loss  4.38 | ppl    79.85\n",
            "| epoch   5 |  2400/ 2713 batches | ms/batch  3.82 | loss  4.37 | ppl    78.87\n",
            "| end of epoch   5 | time: 10.78s | lr [0.5] | valid loss  4.53 | valid ppl    92.78\n",
            "| epoch   6 |   600/ 2713 batches | ms/batch  4.06 | loss  4.41 | ppl    82.46\n",
            "| epoch   6 |  1200/ 2713 batches | ms/batch  3.81 | loss  4.37 | ppl    79.04\n",
            "| epoch   6 |  1800/ 2713 batches | ms/batch  3.81 | loss  4.33 | ppl    75.60\n",
            "| epoch   6 |  2400/ 2713 batches | ms/batch  3.81 | loss  4.32 | ppl    74.90\n",
            "| end of epoch   6 | time: 10.81s | lr [0.5] | valid loss  4.49 | valid ppl    89.25\n",
            "| epoch   7 |   600/ 2713 batches | ms/batch  4.07 | loss  4.36 | ppl    78.31\n",
            "| epoch   7 |  1200/ 2713 batches | ms/batch  3.84 | loss  4.32 | ppl    74.97\n",
            "| epoch   7 |  1800/ 2713 batches | ms/batch  3.82 | loss  4.28 | ppl    71.92\n",
            "| epoch   7 |  2400/ 2713 batches | ms/batch  3.80 | loss  4.27 | ppl    71.45\n",
            "| end of epoch   7 | time: 10.80s | lr [0.5] | valid loss  4.47 | valid ppl    87.15\n",
            "| epoch   8 |   600/ 2713 batches | ms/batch  4.08 | loss  4.31 | ppl    74.72\n",
            "| epoch   8 |  1200/ 2713 batches | ms/batch  3.83 | loss  4.27 | ppl    71.43\n",
            "| epoch   8 |  1800/ 2713 batches | ms/batch  3.83 | loss  4.23 | ppl    68.71\n",
            "| epoch   8 |  2400/ 2713 batches | ms/batch  3.82 | loss  4.23 | ppl    68.39\n",
            "| end of epoch   8 | time: 10.82s | lr [0.5] | valid loss  4.45 | valid ppl    85.76\n",
            "| epoch   9 |   600/ 2713 batches | ms/batch  4.09 | loss  4.27 | ppl    71.54\n",
            "| epoch   9 |  1200/ 2713 batches | ms/batch  3.82 | loss  4.22 | ppl    68.32\n",
            "| epoch   9 |  1800/ 2713 batches | ms/batch  3.82 | loss  4.19 | ppl    65.84\n",
            "| epoch   9 |  2400/ 2713 batches | ms/batch  3.83 | loss  4.18 | ppl    65.62\n",
            "| end of epoch   9 | time: 10.82s | lr [0.5] | valid loss  4.44 | valid ppl    84.80\n",
            "| epoch  10 |   600/ 2713 batches | ms/batch  4.06 | loss  4.23 | ppl    68.66\n",
            "| epoch  10 |  1200/ 2713 batches | ms/batch  3.82 | loss  4.18 | ppl    65.52\n",
            "| epoch  10 |  1800/ 2713 batches | ms/batch  3.81 | loss  4.15 | ppl    63.28\n",
            "| epoch  10 |  2400/ 2713 batches | ms/batch  3.82 | loss  4.14 | ppl    63.11\n",
            "| end of epoch  10 | time: 10.80s | lr [0.5] | valid loss  4.39 | valid ppl    80.89\n",
            "| epoch  11 |   600/ 2713 batches | ms/batch  4.06 | loss  4.19 | ppl    66.05\n",
            "| epoch  11 |  1200/ 2713 batches | ms/batch  3.81 | loss  4.14 | ppl    62.98\n",
            "| epoch  11 |  1800/ 2713 batches | ms/batch  3.81 | loss  4.11 | ppl    60.96\n",
            "| epoch  11 |  2400/ 2713 batches | ms/batch  3.83 | loss  4.11 | ppl    60.86\n",
            "| end of epoch  11 | time: 10.79s | lr [0.5] | valid loss  4.37 | valid ppl    79.04\n",
            "| epoch  12 |   600/ 2713 batches | ms/batch  4.07 | loss  4.15 | ppl    63.69\n",
            "| epoch  12 |  1200/ 2713 batches | ms/batch  3.81 | loss  4.11 | ppl    60.68\n",
            "| epoch  12 |  1800/ 2713 batches | ms/batch  3.82 | loss  4.08 | ppl    58.85\n",
            "| epoch  12 |  2400/ 2713 batches | ms/batch  3.79 | loss  4.07 | ppl    58.79\n",
            "| end of epoch  12 | time: 10.79s | lr [0.5] | valid loss  4.36 | valid ppl    78.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}