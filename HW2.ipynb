{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirmal/tau_dl_proj/blob/master/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDfsYl16STzg",
        "colab_type": "code",
        "outputId": "e749a016-85bd-4aa1-f4b2-52059c396a47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/Deep learning/HW2/PTB.zip\" ./\n",
        "!mkdir -p ./PTB/\n",
        "!unzip -o ./PTB.zip -d ./PTB/\n",
        "cuda = torch.device('cuda')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Archive:  ./PTB.zip\n",
            "  inflating: ./PTB/ptb.char.train.txt  \n",
            "  inflating: ./PTB/ptb.char.valid.txt  \n",
            "  inflating: ./PTB/ptb.test.txt      \n",
            "  inflating: ./PTB/ptb.train.txt     \n",
            "  inflating: ./PTB/ptb.valid.txt     \n",
            "  inflating: ./PTB/README            \n",
            "  inflating: ./PTB/ptb.char.test.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QOApypzGiay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ARGS = {}\n",
        "ARGS['BATCH_SIZE'] = 20\n",
        "ARGS['EPOCHS'] = 20\n",
        "ARGS['BPTT_LEN'] = 20           # sequence length to unroll / backpropegate through time\n",
        "ARGS['HIDDEN_DIM'] = 200        # hidden state vector dimension\n",
        "ARGS['N_LAYERS'] = 2            # Number of hidden layers\n",
        "ARGS['LOG_BATCH_INTVL'] = 2**31 # print training info every <VAL> minibatches, 600 is a reasonable value. set a large\n",
        "                                # value such as 2**31 to disable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1RwjUuKVAJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.datasets import LanguageModelingDataset\n",
        " \n",
        "TEXT = data.Field(lower=True, tokenize='spacy', unk_token='<unk>')\n",
        "\n",
        "ptb_train = LanguageModelingDataset(\"./PTB/ptb.train.txt\", TEXT)\n",
        "ptb_valid = LanguageModelingDataset(\"./PTB/ptb.valid.txt\", TEXT)\n",
        "ptb_test = LanguageModelingDataset(\"./PTB/ptb.test.txt\", TEXT)\n",
        "\n",
        "TEXT.build_vocab(ptb_train)\n",
        "\n",
        "def make_train_iter():\n",
        "  return data.BPTTIterator(\n",
        "    ptb_train,\n",
        "    batch_size=ARGS['BATCH_SIZE'],\n",
        "    bptt_len=ARGS['BPTT_LEN'],\n",
        "    device=cuda,\n",
        "    repeat=False,\n",
        "    shuffle=True)\n",
        "# End\n",
        "\n",
        "\n",
        "def make_valid_iter():\n",
        "  return data.BPTTIterator(\n",
        "    ptb_valid,\n",
        "    batch_size=ARGS['BATCH_SIZE'],\n",
        "    bptt_len=ARGS['BPTT_LEN'],\n",
        "    device=cuda,\n",
        "    repeat=False,\n",
        "    shuffle=True)\n",
        "#End\n",
        "\n",
        "def make_test_iter():\n",
        "  return data.BPTTIterator(\n",
        "    ptb_test,\n",
        "    batch_size=ARGS['BATCH_SIZE'],\n",
        "    bptt_len=ARGS['BPTT_LEN'],\n",
        "    device=cuda,\n",
        "    repeat=False,\n",
        "    shuffle=True)\n",
        "#End\n",
        "\n",
        "\n",
        "#################################\n",
        "## unit test\n",
        "#################################\n",
        "run = False\n",
        "if run is True:\n",
        "  train_iter = make_train_iter()\n",
        "  print(len(train_iter))\n",
        "  for batch in train_iter:\n",
        "    print(batch)\n",
        "    text, target = batch.text, batch.target\n",
        "    text, target = text[:,0], target[:,0]\n",
        "    print(text)\n",
        "    print(len(text))\n",
        "    print(target)\n",
        "    print(len(target))\n",
        "\n",
        "    text_s = [TEXT.vocab.itos[word_idx] for word_idx in text]\n",
        "    target_s = [TEXT.vocab.itos[word_idx] for word_idx in target]\n",
        "\n",
        "    print(text_s)\n",
        "    print(target_s)\n",
        "\n",
        "    break\n",
        "###################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfaXQHAl7UFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyRNN(nn.Module):\n",
        "  def __init__(self, flavour):\n",
        "    super(MyRNN, self).__init__()\n",
        "    if flavour == 'LSTM':\n",
        "      self.rnn_network = torch.nn.LSTM(ARGS['HIDDEN_DIM'], ARGS['HIDDEN_DIM'], ARGS['N_LAYERS'])\n",
        "    elif flavour == 'LSTM_DROPOUT':\n",
        "      # pytorch argument documentation\n",
        "      #   dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
        "      #   LSTM layer except the last layer\n",
        "      # This is as specified in the HW2 requirement, dropout should be applied to the information passing\n",
        "      # between layers and not the information passing between timesteps\n",
        "      self.rnn_network = torch.nn.LSTM(ARGS['HIDDEN_DIM'], ARGS['HIDDEN_DIM'], ARGS['N_LAYERS'], dropout=0.5)\n",
        "    elif flavour == 'GRU':\n",
        "      self.rnn_network = torch.nn.GRU(ARGS['HIDDEN_DIM'], ARGS['HIDDEN_DIM'], ARGS['N_LAYERS'])\n",
        "    elif flavour == 'GRU_DROPOUT':\n",
        "      self.rnn_network = torch.nn.GRU(ARGS['HIDDEN_DIM'], ARGS['HIDDEN_DIM'], ARGS['N_LAYERS'], dropout=0.5)\n",
        "    else:\n",
        "      assert 0\n",
        "\n",
        "    # we need a encoder/decoder to convert to/from one-hot vectors of the vocabulary\n",
        "    self.encoder = nn.Embedding(len(TEXT.vocab), ARGS['HIDDEN_DIM'])\n",
        "    self.encoder.weight.data.uniform_(-0.1, 0.1)\n",
        "    self.decoder = nn.Linear(ARGS['HIDDEN_DIM'], len(TEXT.vocab))\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    self.num_layers = self.rnn_network.num_layers\n",
        "    self.flavour = flavour\n",
        "    # End\n",
        "\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    # https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "    # we can do the entire sequence all at once.\n",
        "    # the first value returned by LSTM is all of the hidden states throughout\n",
        "    # the sequence. the second is just the most recent hidden state\n",
        "    # (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "    # The reason for this is that:\n",
        "    # \"out\" will give you access to all hidden states in the sequence\n",
        "    # \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "    # by passing it as an argument  to the lstm at a later time\n",
        "    # Add the extra 2nd dimension\n",
        "\n",
        "    x = self.encoder(input)\n",
        "    output, hidden = self.rnn_network(x, hidden)\n",
        "    decoded = self.decoder(output)\n",
        "    return decoded, hidden\n",
        "    # End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XHKDSkEIR6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspired by : https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    # Wraps hidden states in new Tensors, to detach them from their history.\n",
        "    #\n",
        "    # detach h,c so we can safely use them as input to next minibatch\n",
        "    # this make sure that gradients dont backprop between minibatches\n",
        "    # https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "# End\n",
        "\n",
        "\n",
        "def train_epoch(model, e, criter, opt, train_iter):\n",
        "  # loss and time variables for logging prints (not used in model training)\n",
        "  total_loss = 0.\n",
        "  start_time = time.time()\n",
        "  model.to(cuda)\n",
        "  model.train() \n",
        "  h = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "  c = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "  hidden = (h, c) if 'LSTM' in model.flavour else (h) # GRU doesnt have long-term state memory\n",
        "  for i, batch in enumerate(train_iter):\n",
        "    data, target = batch.text.to(cuda), batch.target.to(cuda)\n",
        "    model.zero_grad()\n",
        "    for v in hidden: v.to(cuda)\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    output, hidden = model(data, hidden)\n",
        "    loss = criter(output.view(-1, output.shape[-1]), target.view(-1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    total_loss += loss.item()\n",
        "    # log info\n",
        "    if i % ARGS['LOG_BATCH_INTVL'] == 0 and i > 0:\n",
        "      cur_loss = total_loss / ARGS['LOG_BATCH_INTVL']\n",
        "      elapsed = time.time() - start_time\n",
        "      print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "          e, i, len(train_iter), elapsed * 1000 / ARGS['LOG_BATCH_INTVL'], cur_loss, math.exp(cur_loss)))\n",
        "      total_loss = 0\n",
        "      start_time = time.time()\n",
        "# End\n",
        "\n",
        "\n",
        "def lr_scheduler_factor(e):\n",
        "# \"We train it for 4 epochs with a learning rate of 1 and then we decrease the learning rate\n",
        "# by a factor of 2 after each epoch, for a total of 13 training epochs\"\n",
        "  if e < 4:\n",
        "    return 1\n",
        "  elif e > 13:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0.5\n",
        "# End\n",
        "\n",
        "\n",
        "def test(model, test_iter, criter):\n",
        "    model.to(cuda)\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        h = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "        c = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "        hidden = (h, c) if 'LSTM' in model.flavour else (h) # GRU doesnt have long-term state memory\n",
        "        for i, batch in enumerate(test_iter):\n",
        "            data, target = batch.text.to(cuda), batch.target.to(cuda)\n",
        "            for v in hidden: v.to(cuda)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            loss = criter(output.view(-1, output.shape[-1]), target.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(test_iter)\n",
        "# End\n",
        "\n",
        "\n",
        "def train(model):\n",
        "  train_iter = make_train_iter()\n",
        "  valid_iter = make_valid_iter()\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "  lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_scheduler_factor)\n",
        "  for e in range(ARGS['EPOCHS']):\n",
        "    epoch_start_time = time.time()\n",
        "    train_epoch(model, e, criterion, optimizer, train_iter)\n",
        "    val_loss = test(model, valid_iter, criterion)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | lr {:5} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        e, (time.time() - epoch_start_time),\n",
        "        ' '.join(str(v) for v in lr_scheduler.get_lr()),\n",
        "        val_loss, math.exp(val_loss)))\n",
        "    lr_scheduler.step() # update the LR for next epoch\n",
        "# End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGJfD5rDChe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61dcfeaf-c428-491b-dc82-5bb86f72f5f5"
      },
      "source": [
        "flavours = ['LSTM', 'LSTM_DROPOUT', 'GRU', 'GRU_DROPOUT']\n",
        "for flv in flavours:\n",
        "  print('*'*80)\n",
        "  print('*'*20 + ' training ' + flv)\n",
        "  print('*'*80)\n",
        "  net = MyRNN(flv)\n",
        "  train(net)\n",
        "  # run over the test set\n",
        "  test_start_time = time.time()\n",
        "  test_loss = test(net, make_test_iter(), nn.CrossEntropyLoss())\n",
        "  print('testing | time: {:5.2f}s | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "      (time.time() - test_start_time),\n",
        "      test_loss, math.exp(test_loss)))\n",
        "  print('*'*20 + ' Done')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********************************************************************************\n",
            "******************** training LSTM\n",
            "********************************************************************************\n",
            "| end of epoch   0 | time: 11.10s | lr 1     | valid loss  5.24 | valid ppl   189.23\n",
            "| end of epoch   1 | time: 11.10s | lr 1     | valid loss  4.96 | valid ppl   142.16\n",
            "| end of epoch   2 | time: 11.15s | lr 1     | valid loss  4.78 | valid ppl   118.64\n",
            "| end of epoch   3 | time: 11.12s | lr 1     | valid loss  4.66 | valid ppl   105.24\n",
            "| end of epoch   4 | time: 11.07s | lr 0.5   | valid loss  4.57 | valid ppl    96.21\n",
            "| end of epoch   5 | time: 11.17s | lr 0.5   | valid loss  4.53 | valid ppl    93.16\n",
            "| end of epoch   6 | time: 11.06s | lr 0.5   | valid loss  4.49 | valid ppl    89.06\n",
            "| end of epoch   7 | time: 11.07s | lr 0.5   | valid loss  4.47 | valid ppl    87.66\n",
            "| end of epoch   8 | time: 11.07s | lr 0.5   | valid loss  4.43 | valid ppl    84.27\n",
            "| end of epoch   9 | time: 11.09s | lr 0.5   | valid loss  4.42 | valid ppl    83.44\n",
            "| end of epoch  10 | time: 11.13s | lr 0.5   | valid loss  4.39 | valid ppl    80.51\n",
            "| end of epoch  11 | time: 11.13s | lr 0.5   | valid loss  4.38 | valid ppl    79.53\n",
            "| end of epoch  12 | time: 11.10s | lr 0.5   | valid loss  4.36 | valid ppl    77.95\n",
            "| end of epoch  13 | time: 11.05s | lr 0.5   | valid loss  4.34 | valid ppl    76.65\n",
            "| end of epoch  14 | time: 11.03s | lr 1     | valid loss  4.35 | valid ppl    77.43\n",
            "| end of epoch  15 | time: 11.10s | lr 1     | valid loss  4.33 | valid ppl    75.64\n",
            "| end of epoch  16 | time: 11.12s | lr 1     | valid loss  4.31 | valid ppl    74.27\n",
            "| end of epoch  17 | time: 11.12s | lr 1     | valid loss  4.29 | valid ppl    73.27\n",
            "| end of epoch  18 | time: 11.10s | lr 1     | valid loss  4.28 | valid ppl    72.43\n",
            "| end of epoch  19 | time: 11.08s | lr 1     | valid loss  4.28 | valid ppl    72.14\n",
            "| time:  0.35s | test loss  4.17 | test ppl    64.63\n",
            "******************** Done\n",
            "********************************************************************************\n",
            "******************** training LSTM_DROPOUT\n",
            "********************************************************************************\n",
            "| end of epoch   0 | time: 12.48s | lr 1     | valid loss  5.25 | valid ppl   190.22\n",
            "| end of epoch   1 | time: 12.63s | lr 1     | valid loss  4.96 | valid ppl   143.12\n",
            "| end of epoch   2 | time: 12.60s | lr 1     | valid loss  4.80 | valid ppl   121.18\n",
            "| end of epoch   3 | time: 12.63s | lr 1     | valid loss  4.68 | valid ppl   107.62\n",
            "| end of epoch   4 | time: 12.56s | lr 0.5   | valid loss  4.61 | valid ppl   100.55\n",
            "| end of epoch   5 | time: 12.57s | lr 0.5   | valid loss  4.56 | valid ppl    95.96\n",
            "| end of epoch   6 | time: 12.62s | lr 0.5   | valid loss  4.54 | valid ppl    93.32\n",
            "| end of epoch   7 | time: 12.56s | lr 0.5   | valid loss  4.50 | valid ppl    90.38\n",
            "| end of epoch   8 | time: 12.60s | lr 0.5   | valid loss  4.48 | valid ppl    87.84\n",
            "| end of epoch   9 | time: 12.56s | lr 0.5   | valid loss  4.47 | valid ppl    87.73\n",
            "| end of epoch  10 | time: 12.65s | lr 0.5   | valid loss  4.43 | valid ppl    83.76\n",
            "| end of epoch  11 | time: 12.59s | lr 0.5   | valid loss  4.40 | valid ppl    81.63\n",
            "| end of epoch  12 | time: 12.51s | lr 0.5   | valid loss  4.38 | valid ppl    80.24\n",
            "| end of epoch  13 | time: 12.61s | lr 0.5   | valid loss  4.37 | valid ppl    78.67\n",
            "| end of epoch  14 | time: 12.58s | lr 1     | valid loss  4.37 | valid ppl    79.29\n",
            "| end of epoch  15 | time: 12.62s | lr 1     | valid loss  4.35 | valid ppl    77.13\n",
            "| end of epoch  16 | time: 12.62s | lr 1     | valid loss  4.32 | valid ppl    75.39\n",
            "| end of epoch  17 | time: 12.71s | lr 1     | valid loss  4.30 | valid ppl    73.45\n",
            "| end of epoch  18 | time: 12.54s | lr 1     | valid loss  4.28 | valid ppl    72.33\n",
            "| end of epoch  19 | time: 12.49s | lr 1     | valid loss  4.27 | valid ppl    71.30\n",
            "| time:  0.36s | test loss  4.16 | test ppl    64.06\n",
            "******************** Done\n",
            "********************************************************************************\n",
            "******************** training GRU\n",
            "********************************************************************************\n",
            "| end of epoch   0 | time: 10.47s | lr 1     | valid loss  4.93 | valid ppl   137.82\n",
            "| end of epoch   1 | time: 10.41s | lr 1     | valid loss  4.67 | valid ppl   106.61\n",
            "| end of epoch   2 | time: 10.43s | lr 1     | valid loss  4.53 | valid ppl    92.56\n",
            "| end of epoch   3 | time: 10.47s | lr 1     | valid loss  4.43 | valid ppl    84.26\n",
            "| end of epoch   4 | time: 10.41s | lr 0.5   | valid loss  4.33 | valid ppl    76.22\n",
            "| end of epoch   5 | time: 10.42s | lr 0.5   | valid loss  4.30 | valid ppl    74.05\n",
            "| end of epoch   6 | time: 10.49s | lr 0.5   | valid loss  4.28 | valid ppl    72.37\n",
            "| end of epoch   7 | time: 10.43s | lr 0.5   | valid loss  4.26 | valid ppl    71.03\n",
            "| end of epoch   8 | time: 10.48s | lr 0.5   | valid loss  4.25 | valid ppl    69.97\n",
            "| end of epoch   9 | time: 10.43s | lr 0.5   | valid loss  4.24 | valid ppl    69.16\n",
            "| end of epoch  10 | time: 10.48s | lr 0.5   | valid loss  4.23 | valid ppl    68.55\n",
            "| end of epoch  11 | time: 10.49s | lr 0.5   | valid loss  4.22 | valid ppl    68.14\n",
            "| end of epoch  12 | time: 10.51s | lr 0.5   | valid loss  4.22 | valid ppl    67.93\n",
            "| end of epoch  13 | time: 10.45s | lr 0.5   | valid loss  4.22 | valid ppl    67.91\n",
            "| end of epoch  14 | time: 10.47s | lr 1     | valid loss  4.27 | valid ppl    71.28\n",
            "| end of epoch  15 | time: 10.41s | lr 1     | valid loss  4.27 | valid ppl    71.74\n",
            "| end of epoch  16 | time: 10.44s | lr 1     | valid loss  4.28 | valid ppl    72.45\n",
            "| end of epoch  17 | time: 10.49s | lr 1     | valid loss  4.30 | valid ppl    73.44\n",
            "| end of epoch  18 | time: 10.52s | lr 1     | valid loss  4.31 | valid ppl    74.42\n",
            "| end of epoch  19 | time: 10.50s | lr 1     | valid loss  4.33 | valid ppl    75.65\n",
            "| time:  0.35s | test loss  4.21 | test ppl    67.49\n",
            "******************** Done\n",
            "********************************************************************************\n",
            "******************** training GRU_DROPOUT\n",
            "********************************************************************************\n",
            "| end of epoch   0 | time: 11.75s | lr 1     | valid loss  4.95 | valid ppl   141.39\n",
            "| end of epoch   1 | time: 11.73s | lr 1     | valid loss  4.70 | valid ppl   109.61\n",
            "| end of epoch   2 | time: 11.73s | lr 1     | valid loss  4.57 | valid ppl    96.25\n",
            "| end of epoch   3 | time: 11.75s | lr 1     | valid loss  4.47 | valid ppl    87.22\n",
            "| end of epoch   4 | time: 11.81s | lr 0.5   | valid loss  4.37 | valid ppl    79.13\n",
            "| end of epoch   5 | time: 11.85s | lr 0.5   | valid loss  4.34 | valid ppl    76.77\n",
            "| end of epoch   6 | time: 11.74s | lr 0.5   | valid loss  4.32 | valid ppl    74.95\n",
            "| end of epoch   7 | time: 11.74s | lr 0.5   | valid loss  4.30 | valid ppl    73.37\n",
            "| end of epoch   8 | time: 11.74s | lr 0.5   | valid loss  4.27 | valid ppl    71.73\n",
            "| end of epoch   9 | time: 11.73s | lr 0.5   | valid loss  4.26 | valid ppl    70.52\n",
            "| end of epoch  10 | time: 11.73s | lr 0.5   | valid loss  4.24 | valid ppl    69.50\n",
            "| end of epoch  11 | time: 11.76s | lr 0.5   | valid loss  4.23 | valid ppl    68.90\n",
            "| end of epoch  12 | time: 11.80s | lr 0.5   | valid loss  4.22 | valid ppl    68.31\n",
            "| end of epoch  13 | time: 11.76s | lr 0.5   | valid loss  4.21 | valid ppl    67.58\n",
            "| end of epoch  14 | time: 11.73s | lr 1     | valid loss  4.25 | valid ppl    70.38\n",
            "| end of epoch  15 | time: 11.71s | lr 1     | valid loss  4.24 | valid ppl    69.61\n",
            "| end of epoch  16 | time: 11.73s | lr 1     | valid loss  4.23 | valid ppl    68.86\n",
            "| end of epoch  17 | time: 11.70s | lr 1     | valid loss  4.23 | valid ppl    68.43\n",
            "| end of epoch  18 | time: 11.82s | lr 1     | valid loss  4.22 | valid ppl    67.75\n",
            "| end of epoch  19 | time: 11.77s | lr 1     | valid loss  4.21 | valid ppl    67.34\n",
            "| time:  0.36s | test loss  4.10 | test ppl    60.29\n",
            "******************** Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}