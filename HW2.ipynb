{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirmal/tau_dl_proj/blob/master/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDfsYl16STzg",
        "colab_type": "code",
        "outputId": "b77be9e5-ed2b-44ab-af49-dbd147d586e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/Deep learning/HW2/PTB.zip\" ./\n",
        "!mkdir -p ./PTB/\n",
        "!unzip -o ./PTB.zip -d ./PTB/\n",
        "cuda = torch.device('cuda')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Archive:  ./PTB.zip\n",
            "  inflating: ./PTB/ptb.char.train.txt  \n",
            "  inflating: ./PTB/ptb.char.valid.txt  \n",
            "  inflating: ./PTB/ptb.test.txt      \n",
            "  inflating: ./PTB/ptb.train.txt     \n",
            "  inflating: ./PTB/ptb.valid.txt     \n",
            "  inflating: ./PTB/README            \n",
            "  inflating: ./PTB/ptb.char.test.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QOApypzGiay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ARGS = {}\n",
        "ARGS['BATCH_SIZE'] = 20\n",
        "ARGS['EPOCHS'] = 10\n",
        "ARGS['BPTT_LEN'] = 20         # sequence length to unroll / backpropegate through time\n",
        "ARGS['HIDDEN_DIM'] = 200      # hidden state vector dimension\n",
        "ARGS['N_LAYERS'] = 2          # Number of hidden layers\n",
        "ARGS['LOG_BATCH_INTVL'] = 200 # print training info every <VAL> minibatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1RwjUuKVAJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "73a485f8-8814-4c8b-f453-367f669f9827"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.datasets import LanguageModelingDataset\n",
        " \n",
        "TEXT = data.Field(lower=True, tokenize='spacy', unk_token='<unk>')\n",
        "\n",
        "ptb_train = LanguageModelingDataset(\"./PTB/ptb.train.txt\", TEXT)\n",
        "ptb_test = LanguageModelingDataset(\"./PTB/ptb.test.txt\", TEXT)\n",
        "ptb_valid = LanguageModelingDataset(\"./PTB/ptb.valid.txt\", TEXT)\n",
        "\n",
        "TEXT.build_vocab(ptb_train)\n",
        "\n",
        "train_iter = data.BPTTIterator(\n",
        "    ptb_train,\n",
        "    batch_size=ARGS['BATCH_SIZE'],\n",
        "    bptt_len=ARGS['BPTT_LEN'],\n",
        "    device=cuda,\n",
        "    repeat=False,\n",
        "    shuffle=True)\n",
        "\n",
        "#################################\n",
        "## unit test\n",
        "#################################\n",
        "run = True\n",
        "\n",
        "if run is True:\n",
        "  print(len(train_iter))\n",
        "\n",
        "  for batch in train_iter:\n",
        "    print(batch)\n",
        "    text, target = batch.text, batch.target\n",
        "    text, target = text[:,0], target[:,0]\n",
        "    print(text)\n",
        "    print(len(text))\n",
        "    print(target)\n",
        "    print(len(target))\n",
        "\n",
        "    text_s = [TEXT.vocab.itos[word_idx] for word_idx in text]\n",
        "    target_s = [TEXT.vocab.itos[word_idx] for word_idx in target]\n",
        "\n",
        "    print(text_s)\n",
        "    print(target_s)\n",
        "\n",
        "    break\n",
        "###################################"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2713\n",
            "\n",
            "[torchtext.data.batch.Batch of size 20]\n",
            "\t[.text]:[torch.cuda.LongTensor of size 20x20 (GPU 0)]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 20x20 (GPU 0)]\n",
            "tensor([   6, 9703, 9704, 9705, 9707, 9708, 9709, 9712, 9713, 9714, 9715,   23,\n",
            "        5160, 9716, 9718, 9719, 9720, 9721, 9723, 9724], device='cuda:0')\n",
            "20\n",
            "tensor([9703, 9704, 9705, 9707, 9708, 9709, 9712, 9713, 9714, 9715,   23, 5160,\n",
            "        9716, 9718, 9719, 9720, 9721, 9723, 9724, 9725], device='cuda:0')\n",
            "20\n",
            "[' ', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro', '-', 'quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake']\n",
            "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro', '-', 'quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfaXQHAl7UFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLSTM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyLSTM, self).__init__()\n",
        "    self.lstm = torch.nn.LSTM(ARGS['HIDDEN_DIM'], ARGS['HIDDEN_DIM'], ARGS['N_LAYERS'])\n",
        "\n",
        "    # we need a encoder/decoder to convert to/from one-hot vectors of the vocabulary\n",
        "    self.encoder = nn.Embedding(len(TEXT.vocab), ARGS['HIDDEN_DIM'])\n",
        "    self.encoder.weight.data.uniform_(-0.1, 0.1)\n",
        "    self.decoder = nn.Linear(ARGS['HIDDEN_DIM'], len(TEXT.vocab))\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    self.num_layers = self.lstm.num_layers\n",
        "    # End\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    # https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "    # we can do the entire sequence all at once.\n",
        "    # the first value returned by LSTM is all of the hidden states throughout\n",
        "    # the sequence. the second is just the most recent hidden state\n",
        "    # (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "    # The reason for this is that:\n",
        "    # \"out\" will give you access to all hidden states in the sequence\n",
        "    # \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "    # by passing it as an argument  to the lstm at a later time\n",
        "    # Add the extra 2nd dimension\n",
        "\n",
        "    x = self.encoder(input)\n",
        "    output, hidden = self.lstm(x, hidden)\n",
        "    decoded = self.decoder(output)\n",
        "    return decoded, hidden\n",
        "    # End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XHKDSkEIR6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspired by : https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import math\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    # Wraps hidden states in new Tensors, to detach them from their history.\n",
        "    #\n",
        "    # detach h,c so we can safely use them as input to next minibatch\n",
        "    # this make sure that gradients dont backprop between minibatches\n",
        "    # https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train_epoch(model, e, criter, opt):\n",
        "  # loss and time variables for logging prints (not used in model training)\n",
        "  total_loss = 0.\n",
        "  start_time = time.time()\n",
        " \n",
        "  model.to(cuda)\n",
        "  model.train() \n",
        "  h = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "  c = torch.zeros(model.num_layers, ARGS['BATCH_SIZE'], ARGS['HIDDEN_DIM']).to(cuda)\n",
        "  hidden = (h, c)\n",
        "  for i, batch in enumerate(train_iter):\n",
        "    data, target = batch.text, batch.target\n",
        "    data.to(cuda), target.to(cuda)\n",
        "    model.zero_grad()\n",
        "    for v in hidden: v.to(cuda)\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    output, hidden = model(data, hidden)\n",
        "    loss = criter(output.view(-1, output.shape[-1]), target.view(-1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    total_loss += loss.item()\n",
        "    # log info\n",
        "    if i % ARGS['LOG_BATCH_INTVL'] == 0 and i > 0:\n",
        "      cur_loss = total_loss / ARGS['LOG_BATCH_INTVL']\n",
        "      elapsed = time.time() - start_time\n",
        "      print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "          e, i, len(train_iter), elapsed * 1000 / ARGS['LOG_BATCH_INTVL'], cur_loss, math.exp(cur_loss)))\n",
        "      total_loss = 0\n",
        "      start_time = time.time()\n",
        "# End\n",
        "\n",
        "def lr_scheduler_factor(e):\n",
        "# \"We train it for 4 epochs with a learning rate of 1 and then we decrease the learning rate\n",
        "# by a factor of 2 after each epoch, for a total of 13 training epochs\"\n",
        "  if e < 4:\n",
        "    return 1\n",
        "  elif e > 13:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0.5\n",
        "# End\n",
        "\n",
        "def train(model):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "  lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_scheduler_factor)\n",
        "  for e in range(ARGS['EPOCHS']):\n",
        "    epoch_start_time = time.time()\n",
        "    train_epoch(model, e, criterion, optimizer)\n",
        "    lr_scheduler.step() # must be called after optimizer.step()\n",
        "    val_loss = -1 # TODO\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | lr {} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
        "        e, (time.time() - epoch_start_time), lr_scheduler.get_lr(), val_loss, math.exp(val_loss)))\n",
        "\n",
        "# End\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGJfD5rDChe6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a517a7bd-0b70-4868-8cb9-b92a5c395eef"
      },
      "source": [
        "lstm = MyLSTM()\n",
        "\n",
        "train(lstm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   0 |   200/ 2713 batches | ms/batch  5.30 | loss  6.77 | ppl   872.16\n",
            "| epoch   0 |   400/ 2713 batches | ms/batch  4.17 | loss  6.33 | ppl   560.90\n",
            "| epoch   0 |   600/ 2713 batches | ms/batch  4.15 | loss  6.07 | ppl   432.31\n",
            "| epoch   0 |   800/ 2713 batches | ms/batch  4.20 | loss  5.75 | ppl   313.24\n",
            "| epoch   0 |  1000/ 2713 batches | ms/batch  4.23 | loss  5.64 | ppl   281.92\n",
            "| epoch   0 |  1200/ 2713 batches | ms/batch  4.16 | loss  5.55 | ppl   257.41\n",
            "| epoch   0 |  1400/ 2713 batches | ms/batch  4.16 | loss  5.49 | ppl   243.07\n",
            "| epoch   0 |  1600/ 2713 batches | ms/batch  4.14 | loss  5.33 | ppl   207.41\n",
            "| epoch   0 |  1800/ 2713 batches | ms/batch  4.18 | loss  5.34 | ppl   209.22\n",
            "| epoch   0 |  2000/ 2713 batches | ms/batch  4.19 | loss  5.26 | ppl   191.69\n",
            "| epoch   0 |  2200/ 2713 batches | ms/batch  4.17 | loss  5.27 | ppl   193.92\n",
            "| epoch   0 |  2400/ 2713 batches | ms/batch  4.17 | loss  5.25 | ppl   190.36\n",
            "| epoch   0 |  2600/ 2713 batches | ms/batch  4.16 | loss  5.13 | ppl   169.02\n",
            "| end of epoch   0 | time: 11.56s | lr [1] | valid loss -1.00 | valid ppl     0.37\n",
            "| epoch   1 |   200/ 2713 batches | ms/batch  5.22 | loss  5.24 | ppl   187.96\n",
            "| epoch   1 |   400/ 2713 batches | ms/batch  4.18 | loss  5.15 | ppl   172.64\n",
            "| epoch   1 |   600/ 2713 batches | ms/batch  4.25 | loss  5.17 | ppl   175.52\n",
            "| epoch   1 |   800/ 2713 batches | ms/batch  4.12 | loss  5.12 | ppl   167.20\n",
            "| epoch   1 |  1000/ 2713 batches | ms/batch  4.18 | loss  5.10 | ppl   164.62\n",
            "| epoch   1 |  1200/ 2713 batches | ms/batch  4.18 | loss  5.07 | ppl   159.06\n",
            "| epoch   1 |  1400/ 2713 batches | ms/batch  4.17 | loss  5.06 | ppl   157.68\n",
            "| epoch   1 |  1600/ 2713 batches | ms/batch  4.21 | loss  4.93 | ppl   138.54\n",
            "| epoch   1 |  1800/ 2713 batches | ms/batch  4.20 | loss  4.96 | ppl   142.13\n",
            "| epoch   1 |  2000/ 2713 batches | ms/batch  4.12 | loss  4.93 | ppl   138.69\n",
            "| epoch   1 |  2200/ 2713 batches | ms/batch  4.20 | loss  4.92 | ppl   137.38\n",
            "| epoch   1 |  2400/ 2713 batches | ms/batch  4.18 | loss  4.92 | ppl   136.92\n",
            "| epoch   1 |  2600/ 2713 batches | ms/batch  4.19 | loss  4.80 | ppl   121.60\n",
            "| end of epoch   1 | time: 11.56s | lr [1] | valid loss -1.00 | valid ppl     0.37\n",
            "| epoch   2 |   200/ 2713 batches | ms/batch  5.22 | loss  4.92 | ppl   136.81\n",
            "| epoch   2 |   400/ 2713 batches | ms/batch  4.24 | loss  4.85 | ppl   128.05\n",
            "| epoch   2 |   600/ 2713 batches | ms/batch  4.20 | loss  4.89 | ppl   132.38\n",
            "| epoch   2 |   800/ 2713 batches | ms/batch  4.22 | loss  4.83 | ppl   125.63\n",
            "| epoch   2 |  1000/ 2713 batches | ms/batch  4.23 | loss  4.83 | ppl   124.82\n",
            "| epoch   2 |  1200/ 2713 batches | ms/batch  4.16 | loss  4.81 | ppl   122.57\n",
            "| epoch   2 |  1400/ 2713 batches | ms/batch  4.23 | loss  4.81 | ppl   123.13\n",
            "| epoch   2 |  1600/ 2713 batches | ms/batch  4.18 | loss  4.69 | ppl   108.45\n",
            "| epoch   2 |  1800/ 2713 batches | ms/batch  4.22 | loss  4.72 | ppl   112.29\n",
            "| epoch   2 |  2000/ 2713 batches | ms/batch  4.16 | loss  4.72 | ppl   111.79\n",
            "| epoch   2 |  2200/ 2713 batches | ms/batch  4.16 | loss  4.70 | ppl   109.55\n",
            "| epoch   2 |  2400/ 2713 batches | ms/batch  4.15 | loss  4.70 | ppl   109.93\n",
            "| epoch   2 |  2600/ 2713 batches | ms/batch  4.17 | loss  4.59 | ppl    98.40\n",
            "| end of epoch   2 | time: 11.59s | lr [1] | valid loss -1.00 | valid ppl     0.37\n",
            "| epoch   3 |   200/ 2713 batches | ms/batch  5.37 | loss  4.71 | ppl   111.36\n",
            "| epoch   3 |   400/ 2713 batches | ms/batch  4.16 | loss  4.66 | ppl   105.58\n",
            "| epoch   3 |   600/ 2713 batches | ms/batch  4.15 | loss  4.71 | ppl   110.53\n",
            "| epoch   3 |   800/ 2713 batches | ms/batch  4.18 | loss  4.64 | ppl   103.99\n",
            "| epoch   3 |  1000/ 2713 batches | ms/batch  4.21 | loss  4.65 | ppl   104.07\n",
            "| epoch   3 |  1200/ 2713 batches | ms/batch  4.17 | loss  4.64 | ppl   103.38\n",
            "| epoch   3 |  1400/ 2713 batches | ms/batch  4.20 | loss  4.65 | ppl   104.41\n",
            "| epoch   3 |  1600/ 2713 batches | ms/batch  4.16 | loss  4.52 | ppl    92.08\n",
            "| epoch   3 |  1800/ 2713 batches | ms/batch  4.16 | loss  4.57 | ppl    96.19\n",
            "| epoch   3 |  2000/ 2713 batches | ms/batch  4.16 | loss  4.57 | ppl    96.92\n",
            "| epoch   3 |  2200/ 2713 batches | ms/batch  4.19 | loss  4.55 | ppl    94.29\n",
            "| epoch   3 |  2400/ 2713 batches | ms/batch  4.21 | loss  4.55 | ppl    94.20\n",
            "| epoch   3 |  2600/ 2713 batches | ms/batch  4.21 | loss  4.44 | ppl    85.13\n",
            "| end of epoch   3 | time: 11.60s | lr [0.5] | valid loss -1.00 | valid ppl     0.37\n",
            "| epoch   4 |   200/ 2713 batches | ms/batch  5.18 | loss  4.56 | ppl    96.03\n",
            "| epoch   4 |   400/ 2713 batches | ms/batch  4.18 | loss  4.52 | ppl    91.50\n",
            "| epoch   4 |   600/ 2713 batches | ms/batch  4.19 | loss  4.57 | ppl    96.49\n",
            "| epoch   4 |   800/ 2713 batches | ms/batch  4.17 | loss  4.50 | ppl    89.92\n",
            "| epoch   4 |  1000/ 2713 batches | ms/batch  4.25 | loss  4.50 | ppl    90.32\n",
            "| epoch   4 |  1200/ 2713 batches | ms/batch  4.20 | loss  4.50 | ppl    90.40\n",
            "| epoch   4 |  1400/ 2713 batches | ms/batch  4.20 | loss  4.52 | ppl    91.39\n",
            "| epoch   4 |  1600/ 2713 batches | ms/batch  4.22 | loss  4.39 | ppl    80.56\n",
            "| epoch   4 |  1800/ 2713 batches | ms/batch  4.20 | loss  4.44 | ppl    84.92\n",
            "| epoch   4 |  2000/ 2713 batches | ms/batch  4.13 | loss  4.46 | ppl    86.16\n",
            "| epoch   4 |  2200/ 2713 batches | ms/batch  4.18 | loss  4.43 | ppl    83.55\n",
            "| epoch   4 |  2400/ 2713 batches | ms/batch  4.25 | loss  4.42 | ppl    83.20\n",
            "| epoch   4 |  2600/ 2713 batches | ms/batch  4.20 | loss  4.32 | ppl    75.55\n",
            "| end of epoch   4 | time: 11.60s | lr [0.5] | valid loss -1.00 | valid ppl     0.37\n",
            "| epoch   5 |   200/ 2713 batches | ms/batch  5.21 | loss  4.48 | ppl    88.48\n",
            "| epoch   5 |   400/ 2713 batches | ms/batch  4.18 | loss  4.44 | ppl    85.16\n",
            "| epoch   5 |   600/ 2713 batches | ms/batch  4.17 | loss  4.50 | ppl    90.40\n",
            "| epoch   5 |   800/ 2713 batches | ms/batch  4.22 | loss  4.43 | ppl    83.80\n",
            "| epoch   5 |  1000/ 2713 batches | ms/batch  4.19 | loss  4.44 | ppl    84.38\n",
            "| epoch   5 |  1200/ 2713 batches | ms/batch  4.14 | loss  4.44 | ppl    84.78\n",
            "| epoch   5 |  1400/ 2713 batches | ms/batch  4.16 | loss  4.45 | ppl    85.77\n",
            "| epoch   5 |  1600/ 2713 batches | ms/batch  4.18 | loss  4.33 | ppl    75.70\n",
            "| epoch   5 |  1800/ 2713 batches | ms/batch  4.18 | loss  4.38 | ppl    80.15\n",
            "| epoch   5 |  2000/ 2713 batches | ms/batch  4.20 | loss  4.40 | ppl    81.62\n",
            "| epoch   5 |  2200/ 2713 batches | ms/batch  4.24 | loss  4.37 | ppl    78.96\n",
            "| epoch   5 |  2400/ 2713 batches | ms/batch  4.17 | loss  4.36 | ppl    78.37\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}