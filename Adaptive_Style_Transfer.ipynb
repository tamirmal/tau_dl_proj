{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adaptive Style Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirmal/tau_dl_proj/blob/master/Adaptive_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUJvA3lpAHZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "7aa46fee-b3b5-4de3-f344-da24177fc31e"
      },
      "source": [
        "!pip install ipdb\n",
        "import ipdb\n",
        "import datetime, os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision.datasets.mnist import FashionMNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For tensorboard use TF 2.x+\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "GOOGLE_DRIVE_PATH='/content/drive/My Drive/Colab Notebooks/TAU_DL_PROJ/STYLE_TRANSFER/'\n",
        "\n",
        "exists_file_path=GOOGLE_DRIVE_PATH+'exists.file'\n",
        "if not os.path.isfile(GOOGLE_DRIVE_PATH+'exists.file'):\n",
        "  print(\"problem mounting drive FS, failed to access file {}\".format(exists_file_path))\n",
        "  assert 0\n",
        "else:\n",
        "  print(\"successfully accessed drive FS\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/df/78/3d0d7253dc85549db182cbe4b43b30c506c84008fcd39898122c9b6306a9/ipdb-0.12.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (41.4.0)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.7.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (1.12.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.7)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.12.2-cp36-none-any.whl size=9171 sha256=afe1acdf7ca1831d28d350db43f605e65267c8da3afd9257f68cd5d36146710d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/00/07/c906eaf1b90367fbb81bd840e56bf8859dbd3efe3838c0b4ba\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.12.2\n",
            "TensorFlow 2.x selected.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "successfully accessed drive FS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YORkr-pB-LzL",
        "colab_type": "text"
      },
      "source": [
        "![adain_net](https://i.imgur.com/jAyz9hY.jpg)\n",
        "\n",
        "Implementing https://arxiv.org/pdf/1703.06868.pdf\n",
        "There is an official reference in Torch / Lua @ https://github.com/xunhuang1995/AdaIN-style/\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "AdaIN Layer implements the following :\n",
        "\n",
        "![adain_layer](https://i.imgur.com/OiqyfkN.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYATh4Ri_DhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mu_and_sigma(features):\n",
        "    # input is a tensor of shape : [minibatch_size, channels, h ,w]\n",
        "    # output is a tensor of shape : [minibatch_size, channels, 1 ,1]\n",
        "\n",
        "    epsilon = 1e-6\n",
        "    minibatch_size, channels = features.size()[:2]\n",
        "\n",
        "    features_channels_stacked = features.reshape(minibatch_size, channels, -1)\n",
        "\n",
        "    features_mean_per_channel = features_channels_stacked.mean(dim=2)\n",
        "    features_mean_per_channel = features_mean_per_channel.reshape(minibatch_size, channels, 1, 1) # set dim as tensor\n",
        "\n",
        "    features_sigma_per_channel = features_channels_stacked.std(dim=2)\n",
        "    features_sigma_per_channel = features_sigma_per_channel.reshape(minibatch_size, channels, 1, 1) # set dim as tensor\n",
        "\n",
        "    return features_mean_per_channel, features_sigma_per_channel\n",
        "\n",
        "class vgg19_encoder(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(vgg19_encoder, self).__init__()\n",
        "          \n",
        "          encoder = torchvision.models.vgg19(pretrained=True, progress=True)\n",
        "          encoder_layers = list(encoder.children())\n",
        "          relu1_1 = 2\n",
        "          relu2_1 = 7\n",
        "          relu3_1 = 12\n",
        "          relu4_1 = 21\n",
        "          #print(encoder_layers) (I print layers to be sure i extract correct layers...)\n",
        "          \n",
        "          # style encoders - we need to extract intermediate features from SEVERAL layers\n",
        "          # by splitting the model to parts we can take each part output AND feed it into next model part\n",
        "          self.encoder_1 = nn.Sequential(*encoder_layers[:relu1_1])         # input -> relu1_1\n",
        "          self.encoder_2 = nn.Sequential(*encoder_layers[relu1_1:relu2_1])  # relu1_1 -> relu2_1\n",
        "          self.encoder_3 = nn.Sequential(*encoder_layers[relu2_1:relu3_1])  # relu2_1 -> relu3_1\n",
        "          self.encoder_4 = nn.Sequential(*encoder_layers[relu3_1:relu4_1])  # relu3_1 -> relu4_1\n",
        "\n",
        "          # Encoder IS NOT trainable - freeze it\n",
        "          for e in [self.encoder_1, self.encoder_2, self.encoder_3, self.encoder_4]:\n",
        "              for p in e.parameters():\n",
        "                  p.requires_grad = False\n",
        "        # END of __init__()\n",
        "\n",
        "      def forward(self, x, last_only = True):\n",
        "        # last_only : pass only the output of relu4_1 layer\n",
        "        features = [ x ]\n",
        "\n",
        "        features_1 = self.encoder_1(x)\n",
        "        features_2 = self.encoder_2(features_1)\n",
        "        features_3 = self.encoder_3(features_2)\n",
        "        features_4 = self.encoder_4(features_3)\n",
        "        \n",
        "        if last_only is True:\n",
        "          return features_4\n",
        "        else:\n",
        "          features.append(features_1)\n",
        "          features.append(features_2)\n",
        "          features.append(features_3)\n",
        "          features.append(features_4)\n",
        "          features = features[1:]\n",
        "          return features\n",
        "        # END of forward()\n",
        "\n",
        "class style_transfer_net(nn.Module):\n",
        "      def __init__(self):\n",
        "        super(style_transfer_net, self).__init__()\n",
        "\n",
        "        ## using VGG as encoder/decoder\n",
        "        ## TODO : consider using other architectures as suggested in the article\n",
        "        ##        such as resnet34 etc. which are deep BUT have good convergence due to skip-connection (residuals)\n",
        "        \n",
        "        encoder_t = 'VGG19' # TODO in the future this will be external argument\n",
        "        if encoder_t == 'VGG19':\n",
        "          encoder = torchvision.models.vgg19(pretrained=True, progress=True)\n",
        "          encoder_layers = list(encoder.children())\n",
        "          relu1_1 = 2\n",
        "          relu2_1 = 7\n",
        "          relu3_1 = 12\n",
        "          relu4_1 = 21\n",
        "          #print(encoder_layers) (I print layers to be sure i extract correct layers...)\n",
        "          \n",
        "          # style encoders - we need to extract intermediate features from SEVERAL layers\n",
        "          # by splitting the model to parts we can take each part output AND feed it into next model part\n",
        "          self.encoder_style1 = nn.Sequential(*encoder_layers[:relu1_1])         # input -> relu1_1\n",
        "          self.encoder_style2 = nn.Sequential(*encoder_layers[relu1_1:relu2_1])  # relu1_1 -> relu2_1\n",
        "          self.encoder_style3 = nn.Sequential(*encoder_layers[relu2_1:relu3_1])  # relu2_1 -> relu3_1\n",
        "          self.encoder_style4 = nn.Sequential(*encoder_layers[relu3_1:relu4_1])  # relu3_1 -> relu4_1\n",
        "          # content encoder (needs to be concatenated to encoder_style3)\n",
        "          self.encoder_content = nn.Sequential(*encoder_layers[relu3_1:relu4_1]) # relu3_1 -> relu4_1\n",
        "\n",
        "          # Encoder IS NOT trainable - freeze it\n",
        "          for e in [self.encoder_style1, self.encoder_style2, self.encoder_style3, self.encoder_style4, self.encoder_content]:\n",
        "              for p in e.parameters():\n",
        "                  p.requires_grad = False\n",
        "\n",
        "        decoder_t = 'VGG19'\n",
        "        if decoder_t == 'VGG19':\n",
        "          self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "          )\n",
        "\n",
        "      # End\n",
        "      def adain_layer(content_features, style_features):\n",
        "          # Adaptive instance normalization\n",
        "          # Inputs are :\n",
        "          #  content features - the content image output from VGG_ENCODER.relu4_1     [batch_size, 512, h/8, w/8]\n",
        "          #  style features   - the style image output from VGG_ENCODER.relu4_1       [batch_size, 512, h/8, w/8]\n",
        "          # Therefore they have the same dimensions of 512x(H/8)x(W/8)\n",
        "          # This layer calculates a per-channel mean and std of the style features\n",
        "          # and scales the content features so they have the same mean and std (per channel) of the style\n",
        "          \n",
        "          content_mu, content_sigma = get_mu_and_sigma(content_features)\n",
        "          style_mu, style_sigma = get_mu_and_sigma(style_features)\n",
        "\n",
        "          normalized_content_features = (content_features - content_mu) / content_sigma\n",
        "          style_normalized_content_features = style_sigma*normalized_content_features + style_mu\n",
        "          return style_normalized_content_features\n",
        "\n",
        "\n",
        "      @staticmethod\n",
        "      def calc_content_loss(out_features, t):\n",
        "          return F.mse_loss(out_features, t)\n",
        "\n",
        "      @staticmethod\n",
        "      def calc_style_loss(content_middle_features, style_middle_features):\n",
        "          loss = 0\n",
        "          for c, s in zip(content_middle_features, style_middle_features):\n",
        "              c_mean, c_std = calc_mean_std(c)\n",
        "              s_mean, s_std = calc_mean_std(s)\n",
        "              loss += F.mse_loss(c_mean, s_mean) + F.mse_loss(c_std, s_std)\n",
        "          return loss\n",
        "\n",
        "\n",
        "      def forward(self, content, style, alpha=1.0):\n",
        "        assert alpha >= 0\n",
        "        assert alpha <= 1\n",
        "        # TODO - add asserts that encoders are NOT trainable !!!\n",
        "\n",
        "        ###########################################\n",
        "        # Encoder pass of content and style images\n",
        "        ###########################################\n",
        "\n",
        "        #\n",
        "        #  ENC1 --- ENC2 --- ENC3 --- ENC4 ---\n",
        "        #        |        |        |        |\n",
        "        #       feat1    feat2    feat3    feat4\n",
        "        style_features = [ style ]\n",
        "        style1 = self.encoder_style1(style)\n",
        "        style2 = self.encoder_style2(style1)\n",
        "        style3 = self.encoder_style3(style2)\n",
        "        style4 = self.encoder_style4(style3)     \n",
        "        style_features.append(style1)\n",
        "        style_features.append(style2)\n",
        "        style_features.append(style3)\n",
        "        style_features.append(style4)\n",
        "        style_features = style_features[1:]\n",
        "        # content features extracted from feat4 (but content is input, not style)\n",
        "        content_features = self.encoder_style1(content)\n",
        "        content_features = self.encoder_style2(content_features)\n",
        "        content_features = self.encoder_style3(content_features)\n",
        "        content_features = self.encoder_content(content_features)\n",
        "\n",
        "        ###########################################\n",
        "        # AdaIn step\n",
        "        ###########################################\n",
        "        # feed into AdaIn layer the style & content features, get style-normalized content features\n",
        "        style_norm_content = adain_layer(content_features, style4)\n",
        "        style_norm_content = alpha*style_norm_content + (1-alpha)*content_features # hyper-parameter, a tradeoff between content and style\n",
        "        \n",
        "        ###########################################\n",
        "        # Apply the style transfer\n",
        "        ###########################################\n",
        "        # pass through decoder, obtain transformed image\n",
        "        out = self.decoder(style_norm_content)\n",
        "\n",
        "        ###########################################\n",
        "        # Loss calculation\n",
        "        ###########################################\n",
        "        # get content & style features of output image (after style transfer), same process as above\n",
        "        # TODO - enclose this in a function, too much repeating code\n",
        "        out_content_features = self.encoder_style1(out)\n",
        "        out_content_features = self.encoder_style2(out_content_features)\n",
        "        out_content_features = self.encoder_style3(out_content_features)\n",
        "        out_content_features = self.encoder_style4(out_content_features)\n",
        "        # get style features\n",
        "        out_style = [ style ]\n",
        "        out_style1 = self.encoder_style1(out)\n",
        "        out_style2 = self.encoder_style2(out_style1)\n",
        "        out_style3 = self.encoder_style3(out_style2)\n",
        "        out_style4 = self.encoder_style4(out_style3)\n",
        "        out_style.append(out_style1)\n",
        "        out_style.append(out_style2)\n",
        "        out_style.append(out_style3)\n",
        "        out_style.append(out_style4)\n",
        "        out_style = style_features[1:]\n",
        "\n",
        "\n",
        "      # End\n",
        "\n",
        "model = style_transfer_net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvORSLClWhe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}