{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adaptive Style Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirmal/tau_dl_proj/blob/master/Adaptive_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RICyQCfMmGAI",
        "colab_type": "text"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "502U9xjXl8Yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train or Evaluate\n",
        "_MODE = 'resume_train' #@param ['train', 'eval', 'resume_train']\n",
        "# Load models if resuming training or evaluating existing model\n",
        "_resume_train_from = 'final_iter_79999.pth' #@param {type: \"string\"}\n",
        "_eval_model_path = 'split_1_final_iters_40000.pth' #@param {type: \"string\"}\n",
        "# Where dataset is located, its name\n",
        "_dataset_backend = 'gdrive'  #@param ['dropbox', 'gdrive', 'gbucket']\n",
        "_style_dataset_name = 'wikiart_3.zip' #@param ['wikiart_1.zip', 'wikiart_2.zip', 'wikiart_3.zip', 'wikiart_4.zip']\n",
        "_content_dataset_name = 'train2014_coco_3.zip' #@param ['train2014_coco_1.zip', 'train2014_coco_2.zip', 'train2014_coco_3.zip', 'train2014_coco_4.zip']\n",
        "# Optimizer settings\n",
        "_optimizer = 'adam' #@param ['adam', 'SGD']\n",
        "_lr_decay = 0 #@param {type:\"number\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUDhct2mmLzP",
        "colab_type": "text"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUJvA3lpAHZQ",
        "colab_type": "code",
        "outputId": "21e2a153-ec17-4993-f352-53fedb805ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "#!pip install ipdb\n",
        "#import ipdb\n",
        "!pip install piexif\n",
        "\n",
        "\n",
        "import datetime, os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "GLOBAL_ARGS = {}\n",
        "GLOBAL_ARGS['dataset_backend'] = _dataset_backend\n",
        "GLOBAL_ARGS['lr'] = 5e-5\n",
        "GLOBAL_ARGS['lr_decay'] = _lr_decay\n",
        "GLOBAL_ARGS['momentum'] = 0.9\n",
        "GLOBAL_ARGS['optimizer'] = _optimizer\n",
        "GLOBAL_ARGS['batch_size'] = 8\n",
        "GLOBAL_ARGS['max_iters'] = 160000\n",
        "GLOBAL_ARGS['max_iters_per_split'] = GLOBAL_ARGS['max_iters'] // 4\n",
        "GLOBAL_ARGS['loss_style_w'] = 10 # Weight of style loss\n",
        "GLOBAL_ARGS['loss_content_w'] = 1 # Weight of content loss\n",
        "GLOBAL_ARGS['MODE'] = _MODE\n",
        "GLOBAL_ARGS['resume_train_from'] = _resume_train_from\n",
        "GLOBAL_ARGS['eval_model_path'] = _eval_model_path\n",
        "GLOBAL_ARGS['style_dataset_name'] = _style_dataset_name\n",
        "GLOBAL_ARGS['content_dataset_name'] = _content_dataset_name\n",
        "\n",
        "\"\"\"\n",
        "TAMIR:arguments from authors LUA code that are not used in my code :\n",
        "-- Training options\n",
        "cmd:option('-resume', false, 'If true, resume training from the last checkpoint')\n",
        "cmd:option('-weightDecay', 0, 'Weight decay')\n",
        "cmd:option('-targetContentLayer', 'relu4_1', 'Target content layer used to compute the loss')\n",
        "cmd:option('-targetStyleLayers', 'relu1_1,relu2_1,relu3_1,relu4_1', 'Target style layers used to compute the loss')\n",
        "cmd:option('-tvWeight', 0, 'Weight of TV loss')\n",
        "cmd:option('-reconStyle', false, 'If true, the decoder is also trained to reconstruct style images')\n",
        "cmd:option('-normalize', false, 'If true, gradients at the loss function are normalized')\n",
        "\"\"\"\n",
        "\n",
        "# check mode is valid\n",
        "assert GLOBAL_ARGS['MODE'] in ['train', 'eval', 'resume_train']\n",
        "#### Mount Google Drive\n",
        "from google.colab import drive\n",
        "if GLOBAL_ARGS['dataset_backend'] == 'gdrive' or GLOBAL_ARGS['dataset_backend'] == 'dropbox':\n",
        "  drive.mount('/gdrive')\n",
        "  model_save_path = '/gdrive/My Drive/Colab Notebooks/adaptive_style_transfer/models/'\n",
        "\n",
        "#### Mount GCP bucket\n",
        "from google.colab import auth\n",
        "if GLOBAL_ARGS['dataset_backend'] == 'gbucket':\n",
        "  auth.authenticate_user()\n",
        "  project_id = 'tau-dl'\n",
        "  !gcloud config set project {project_id}\n",
        "  !gsutil ls\n",
        "  !echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "  !curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "  !apt -qq update\n",
        "  !apt -qq install gcsfuse\n",
        "  !mkdir -p /content/adaptive_style_transfer/\n",
        "  !gcsfuse --implicit-dirs adaptive_style_transfer /content/adaptive_style_transfer/\n",
        "\n",
        "# For tensorboard use TF 2.x+\n",
        "%tensorflow_version 2.x\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  cpu = torch.device(\"cpu\")\n",
        "else:\n",
        "  # no GPU available\n",
        "  assert 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting piexif\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/d8/6f63147dd73373d051c5eb049ecd841207f898f50a5a1d4378594178f6cf/piexif-1.1.3-py2.py3-none-any.whl\n",
            "Installing collected packages: piexif\n",
            "Successfully installed piexif-1.1.3\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5eFQRfUinsw",
        "colab_type": "text"
      },
      "source": [
        "# Dataset preparation - download or list, define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNrQDdajUUba",
        "colab_type": "code",
        "outputId": "db306837-3460-4c19-88e7-e11302c23585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "##### manual calculation of entire dataset length\n",
        "## Number of content examples:\n",
        "## 82783 /root/content_examples_paths.lst\n",
        "## Number of style examples:\n",
        "## 79433 /root/style_examples_paths.lst\n",
        "##\n",
        "## gbucket mode - images are stored on a GCP bucket (http access), we cant do 'ls' to get the file list\n",
        "##                so I've manually creates a list by mounting the GCP bucket as a file-system and used 'gsutil ls'\n",
        "##                The list is stored in the GCP bucket with the images\n",
        "## gdrive  mode - dataset is split to 4 parts in ZIP files, files are copied from gdrive and unzipped locally\n",
        "##\n",
        "\n",
        "\n",
        "# Function that takes a path to a dataset file, path to extract the dataset into\n",
        "# copies it to local storage\n",
        "# unzip\n",
        "# removes the .zip archive from local storage\n",
        "def copy_and_unzip_dataset(F, D):\n",
        "      if os.path.isdir(D):\n",
        "        print(\"found {} directory, skipping\".format(D))\n",
        "        return\n",
        "\n",
        "      if os.path.isfile(F) == False:\n",
        "        print(\"copying {}\".format(F))\n",
        "        !date\n",
        "#        !cp \"/gdrive/My Drive/Colab Notebooks/adaptive_style_transfer/$DATASET_FILE\" ./\n",
        "        !rsync --progress \"/gdrive/My Drive/Colab Notebooks/adaptive_style_transfer/$DATASET_FILE\" ./\n",
        "        !date\n",
        "      else:\n",
        "        print(\"{} already exists locally\".format(F))\n",
        "\n",
        "      # unzipping\n",
        "      print(\"unzipping {} to {}\".format(F, D))\n",
        "      !date\n",
        "      !7z e ${DATASET_FILE} -o${DATASET_DIR}\n",
        "      !date\n",
        "      print(\"done unzipping, removing zip archive\")\n",
        "      !rm -fr ${DATASET_FILE}\n",
        "# End\n",
        "\n",
        "\n",
        "def download_and_unzip_dataset(F, D):\n",
        "      if 'wikiart' in F:\n",
        "        url = 'https://dl.dropbox.com/s/uu1g98dsq5mw3x2/' + GLOBAL_ARGS['style_dataset_name'] +'?dl=1'\n",
        "      elif 'coco' in F:\n",
        "        url = 'https://dl.dropbox.com/s/nh8lxx3nnex1a1y/' + GLOBAL_ARGS['content_dataset_name'] +'?dl=1'\n",
        "\n",
        "      if os.path.isdir(D):\n",
        "        print(\"found {} directory, skipping\".format(D))\n",
        "        return\n",
        "\n",
        "      if os.path.isfile(F) == False:\n",
        "        print(\"downloading {}\".format(F))\n",
        "        !date\n",
        "        os.environ['URL'] = url\n",
        "        !wget -O ${DATASET_FILE} ${URL}\n",
        "        !date\n",
        "      else:\n",
        "        print(\"{} already exists locally\".format(F))\n",
        "\n",
        "      # unzipping\n",
        "      print(\"unzipping {} to {}\".format(F, D))\n",
        "      !date\n",
        "      !7z e ${DATASET_FILE} -o${DATASET_DIR}\n",
        "      !date\n",
        "      print(\"done unzipping, removing zip archive\")\n",
        "      !rm -fr ${DATASET_FILE}\n",
        "# End\n",
        "\n",
        "\n",
        "if GLOBAL_ARGS['dataset_backend'] == 'gbucket':\n",
        "  ##### Generate the file list for dataloaders\n",
        "  #!gsutil ls gs://adaptive_style_transfer/COCO_SPLITS/*/*.jpg > /content/adaptive_style_transfer/content_examples_paths.lst\n",
        "  #!gsutil ls gs://adaptive_style_transfer/WIKIART_SPLITS/*/*.jpg > /content/adaptive_style_transfer/style_examples_paths.lst\n",
        "\n",
        "  !gsutil cp gs://adaptive_style_transfer/content_examples_paths.lst /root/\n",
        "  !echo \"Number of content examples:\"\n",
        "  !wc -l /root/content_examples_paths.lst\n",
        "\n",
        "  !gsutil cp gs://adaptive_style_transfer/style_examples_paths.lst /root/\n",
        "  !echo \"Number of style examples:\"\n",
        "  !wc -l /root/style_examples_paths.lst\n",
        "elif GLOBAL_ARGS['dataset_backend'] == 'gdrive' or GLOBAL_ARGS['dataset_backend'] == 'dropbox':\n",
        "  import os\n",
        "  wikiart_file = './' + GLOBAL_ARGS['style_dataset_name']\n",
        "  mscoco_file = './' + GLOBAL_ARGS['content_dataset_name']\n",
        "\n",
        "  for FILE in [wikiart_file, mscoco_file]:\n",
        "    # set dataset folder, content or style ?\n",
        "    if 'wikiart' in FILE:\n",
        "      D = './dataset_style'\n",
        "      O = '/root/style_examples_paths.lst'\n",
        "    elif 'coco' in FILE:\n",
        "      D = './dataset_content'\n",
        "      O = '/root/content_examples_paths.lst'\n",
        "    # export env vars\n",
        "    os.environ['DATASET_FILE'] = FILE\n",
        "    os.environ['DATASET_DIR'] = D\n",
        "    os.environ['DATASET_LST'] = O\n",
        "    # copy, extract and remove archive\n",
        "    if GLOBAL_ARGS['dataset_backend'] == 'gdrive':\n",
        "      copy_and_unzip_dataset(FILE, D)\n",
        "    else:\n",
        "      download_and_unzip_dataset(FILE, D)\n",
        "    # Now list the files\n",
        "    !find ${DATASET_DIR} -name \"*.jpg\" > ${DATASET_LST}\n",
        "    !echo \"number of examples in this split:\"\n",
        "    !wc -l ${DATASET_LST}\n",
        "else:\n",
        "  assert 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying ./wikiart_3.zip\n",
            "Mon Dec 30 18:37:52 UTC 2019\n",
            "wikiart_3.zip\n",
            "  2,437,185,536  24%    4.02kB/s  518:43:53  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgAqs_HVkGCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "from PIL import Image, ImageFile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from google.cloud import storage\n",
        "import piexif\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "Image.MAX_IMAGE_PIXELS = 1000000000                                                                                              \n",
        "\n",
        "class DatasetFolders(data.Dataset):\n",
        "    def __init__(self, files_paths, transform):\n",
        "        super(DatasetFolders, self).__init__()\n",
        "        with open(files_paths) as F:\n",
        "          paths = F.readlines()\n",
        "        self.paths = [x.strip() for x in paths] \n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "\n",
        "        if GLOBAL_ARGS['dataset_backend'] == 'gbucket':\n",
        "          # GCS backend\n",
        "          # PROBLEM - because the dataloaders are multi-threaded but GCP client/buckets library is not thread-safe\n",
        "          # I'm doing this\n",
        "          #    client = ...\n",
        "          #    bucket = ...\n",
        "          # per-image \n",
        "          # so we need to create an instance per-thread.\n",
        "          # adding this to the TODO list ...\n",
        "          client = storage.Client(project=project_id)\n",
        "          bucket = client.get_bucket('adaptive_style_transfer')\n",
        "          blob_path = path.split('adaptive_style_transfer')[-1]\n",
        "          blob_path = blob_path[1:] # first char is '/', trim it ...\n",
        "          blob = bucket.get_blob(blob_path).download_as_string()\n",
        "          bytes = BytesIO(blob)\n",
        "          img = Image.open(bytes).convert('RGB')\n",
        "        else:\n",
        "          try:\n",
        "            piexif.remove(path)\n",
        "          except Exception:\n",
        "            pass\n",
        "          img = Image.open(path).convert('RGB')\n",
        "        # End\n",
        "\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def name(self):\n",
        "        return 'DatasetFolders'\n",
        "\n",
        "\n",
        "trans = [\n",
        "    transforms.Resize(size=(512, 512)),\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "]\n",
        "trans = transforms.Compose(trans)\n",
        "\n",
        "content_dataset = DatasetFolders('/root/content_examples_paths.lst', trans)\n",
        "style_dataset = DatasetFolders('/root/style_examples_paths.lst', trans)\n",
        "\n",
        "# denorm (defined here to be close to the norm transformation)\n",
        "def denorm(x):\n",
        "    mu = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1).to(device)\n",
        "    sigma = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1).to(device)\n",
        "    return torch.clamp(x * sigma + mu, 0, 1)\n",
        "# End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp8FnasfYpN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sd = 999\n",
        "def torch_seed():\n",
        "    global sd\n",
        "    torch.manual_seed(sd)\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(sd)\n",
        "\n",
        "## Training will not be in epochs but in iterations\n",
        "## the dataloaders will need to be infinite\n",
        "## (in epoch we finish an epoch at end of iteration)\n",
        "## (doing as in the article/lua-torch code. training for 160,000 iteration with batch size of 8)\n",
        "\n",
        "# the built in samplers in pytorch are not infinite, defining my own\n",
        "def InfiniteSampler(dataset_len):\n",
        "    torch_seed()\n",
        "    perm_iter = iter(torch.randperm(dataset_len).tolist())\n",
        "    while True:\n",
        "      try:\n",
        "        yield perm_iter.__next__()\n",
        "      except StopIteration:\n",
        "        perm_iter = iter(torch.randperm(dataset_len).tolist())\n",
        "        yield perm_iter.__next__()\n",
        "\n",
        "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
        "    def __init__(self, data_source):\n",
        "        self.num_samples = len(data_source)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(InfiniteSampler(self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "def make_content_iter(batch_size=8, num_workers=8):\n",
        "  return iter(data.DataLoader(\n",
        "      content_dataset, batch_size=batch_size,\n",
        "      sampler=InfiniteSamplerWrapper(content_dataset),\n",
        "      num_workers=num_workers))\n",
        "# End\n",
        "\n",
        "def make_style_iter(batch_size=8, num_workers=8):\n",
        "  return iter(data.DataLoader(\n",
        "      style_dataset, batch_size=batch_size,\n",
        "      sampler=InfiniteSamplerWrapper(style_dataset),\n",
        "      num_workers=num_workers))\n",
        "\n",
        "########################################\n",
        "### Unit test\n",
        "########################################\n",
        "run = False\n",
        "if run == True:\n",
        "  content_iter = make_content_iter(1, 1)\n",
        "  images = content_iter.next()\n",
        "  print('images shape on batch size = {}'.format(images.size()))\n",
        "  grid = torchvision.utils.make_grid(images)\n",
        "  plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "  print(images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRVFasH4ihSl",
        "colab_type": "text"
      },
      "source": [
        "# Network Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YORkr-pB-LzL",
        "colab_type": "text"
      },
      "source": [
        "![adain_net](https://i.imgur.com/jAyz9hY.jpg)\n",
        "\n",
        "Implementing https://arxiv.org/pdf/1703.06868.pdf\n",
        "There is an official reference in Torch / Lua @ https://github.com/xunhuang1995/AdaIN-style/\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "AdaIN Layer implements the following :\n",
        "\n",
        "![adain_layer](https://i.imgur.com/OiqyfkN.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYATh4Ri_DhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def get_mu_and_sigma(features):\n",
        "    # input is a tensor of shape : [minibatch_size, channels, h ,w]\n",
        "    # output is a tensor of shape : [minibatch_size, channels, 1 ,1]\n",
        "\n",
        "    epsilon = 1e-6\n",
        "    minibatch_size, channels = features.size()[:2]\n",
        "\n",
        "    features_channels_stacked = features.reshape(minibatch_size, channels, -1)\n",
        "\n",
        "    features_mean_per_channel = features_channels_stacked.mean(dim=2) # dim 0 : minibatch, dim 1 : channels, !! dim 2 : features col stack !!\n",
        "    features_mean_per_channel = features_mean_per_channel.reshape(minibatch_size, channels, 1, 1) # set dim as tensor\n",
        "\n",
        "    features_sigma_per_channel = features_channels_stacked.std(dim=2)\n",
        "    features_sigma_per_channel = features_sigma_per_channel.reshape(minibatch_size, channels, 1, 1) # set dim as tensor\n",
        "    # Notice! since we will do normalization (divide by sigma) - we add a small epsilon for numerical stablity (avoid NaN's or Inf's)\n",
        "    return features_mean_per_channel, features_sigma_per_channel + epsilon\n",
        "\n",
        "class vgg19_encoder(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(vgg19_encoder, self).__init__()\n",
        "          \n",
        "          encoder = torchvision.models.vgg19(pretrained=True, progress=True)\n",
        "          print(encoder) # print encoder, to make sure i'm extracting the correct layers\n",
        "          encoder_layers = list(encoder.features.children())\n",
        "          relu1_1 = 2\n",
        "          relu2_1 = 7\n",
        "          relu3_1 = 12\n",
        "          relu4_1 = 21\n",
        "          \n",
        "          # style encoders - we need to extract intermediate features from SEVERAL layers\n",
        "          # by splitting the model to parts we can take each part output AND feed it into next model part\n",
        "          self.encoder_1 = nn.Sequential(*encoder_layers[:relu1_1])         # input -> relu1_1\n",
        "          self.encoder_2 = nn.Sequential(*encoder_layers[relu1_1:relu2_1])  # relu1_1 -> relu2_1\n",
        "          self.encoder_3 = nn.Sequential(*encoder_layers[relu2_1:relu3_1])  # relu2_1 -> relu3_1\n",
        "          self.encoder_4 = nn.Sequential(*encoder_layers[relu3_1:relu4_1])  # relu3_1 -> relu4_1\n",
        "\n",
        "          # Encoder IS NOT trainable - freeze it\n",
        "          for e in [self.encoder_1, self.encoder_2, self.encoder_3, self.encoder_4]:\n",
        "              for p in e.parameters():\n",
        "                  p.requires_grad = False\n",
        "      # END of __init__()\n",
        "\n",
        "      def forward(self, x, last_only = True):\n",
        "        #\n",
        "        #  ENC1 --- ENC2 --- ENC3 --- ENC4 ---\n",
        "        #        |        |        |        |\n",
        "        #     relu1_1    relu2_1  relu3_1  relu4_1\n",
        "\n",
        "        # last_only : pass only the output of relu4_1 layer\n",
        "\n",
        "        # init a list with X in it, X will be discarded later and is only used to init a list of tensors\n",
        "        features = [ x ]\n",
        "\n",
        "        features_1 = self.encoder_1(x)\n",
        "        features_2 = self.encoder_2(features_1)\n",
        "        features_3 = self.encoder_3(features_2)\n",
        "        features_4 = self.encoder_4(features_3)\n",
        "        \n",
        "        if last_only is True:\n",
        "          return features_4\n",
        "        else:\n",
        "          features.append(features_1)\n",
        "          features.append(features_2)\n",
        "          features.append(features_3)\n",
        "          features.append(features_4)\n",
        "          features = features[1:] # discards the X\n",
        "          return features\n",
        "      # END of forward()\n",
        "\n",
        "\n",
        "vgg19_decoder = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "          )\n",
        "\n",
        "class style_transfer_net(nn.Module):\n",
        "      def __init__(self):\n",
        "        super(style_transfer_net, self).__init__()\n",
        "\n",
        "        ## using VGG as encoder/decoder\n",
        "        ## TODO : consider using other architectures as suggested in the article\n",
        "        ##        such as resnet34 etc. which are deep BUT have good convergence due to skip-connection (residuals)\n",
        "        \n",
        "        encoder_t = 'VGG19' # TODO in the future this will be external argument\n",
        "        if encoder_t == 'VGG19':\n",
        "          self.encoder = vgg19_encoder()\n",
        "\n",
        "        decoder_t = 'VGG19'\n",
        "        if decoder_t == 'VGG19':\n",
        "          self.decoder = vgg19_decoder\n",
        "      # End\n",
        "\n",
        "      @staticmethod\n",
        "      def adain_layer(content_features, style_features):\n",
        "          # Adaptive instance normalization\n",
        "          # Inputs are :\n",
        "          #  content features - the content image output from VGG_ENCODER.relu4_1     [batch_size, 512, h/8, w/8]\n",
        "          #  style features   - the style image output from VGG_ENCODER.relu4_1       [batch_size, 512, h/8, w/8]\n",
        "          # Therefore they have the same dimensions of 512x(H/8)x(W/8)\n",
        "          # This layer calculates a per-channel mean and std of the style features\n",
        "          # and scales the content features so they have the same mean and std (per channel) of the style\n",
        "          \n",
        "          content_mu, content_sigma = get_mu_and_sigma(content_features)\n",
        "          style_mu, style_sigma = get_mu_and_sigma(style_features)\n",
        "\n",
        "          normalized_content_features = (content_features - content_mu) / content_sigma\n",
        "          style_normalized_content_features = style_sigma*normalized_content_features + style_mu\n",
        "          return style_normalized_content_features\n",
        "      # End\n",
        "\n",
        "      @staticmethod\n",
        "      def calc_content_loss(out_content, adain_content):\n",
        "          return F.mse_loss(out_content, adain_content)\n",
        "      # End\n",
        "\n",
        "      @staticmethod\n",
        "      def calc_style_loss(out_style, in_style):\n",
        "          loss = 0\n",
        "          for a,b in zip(out_style, in_style):\n",
        "              a_mu, a_sigma = get_mu_and_sigma(a)\n",
        "              b_mu, b_sigma = get_mu_and_sigma(b)\n",
        "              loss += F.mse_loss(a_mu, b_mu) + F.mse_loss(a_sigma, b_sigma)\n",
        "          return loss\n",
        "      # End\n",
        "\n",
        "      def forward(self, content, style, alpha=1.0):\n",
        "        assert alpha >= 0\n",
        "        assert alpha <= 1\n",
        "        # TODO - add asserts that encoders are NOT trainable !!!\n",
        "\n",
        "        ###########################################\n",
        "        # Encoder pass of content and style images\n",
        "        ###########################################\n",
        "        style_features = self.encoder(style, last_only=False)   # for VGG19 [relu1_1, relu2_1, relu3_1, relu4_1]\n",
        "        content_features = self.encoder(content, last_only=True)  # for VGG19 relu4_1\n",
        "\n",
        "        ###########################################\n",
        "        # AdaIn step\n",
        "        ###########################################\n",
        "        # feed into AdaIn layer the style & content features, get style-normalized content features\n",
        "        style4=style_features[-1]\n",
        "        style_norm_content = self.adain_layer(content_features, style4)\n",
        "        style_norm_content = alpha*style_norm_content + (1-alpha)*content_features # hyper-parameter, a tradeoff between content and style\n",
        "\n",
        "        ###########################################\n",
        "        # Apply Style Transfer :\n",
        "        # Enter the style-normalized-content features to the decoder\n",
        "        # output is the transformed image\n",
        "        ###########################################\n",
        "        # pass through decoder, obtain transformed image\n",
        "        out = self.decoder(style_norm_content)\n",
        "\n",
        "        ###########################################\n",
        "        # Loss calculation\n",
        "        ###########################################\n",
        "        # get content & style features of output image (after style transfer), same process as above\n",
        "        # TODO - enclose this in a function, too much repeating code\n",
        "        out_content_features = self.encoder(out, last_only=True)  # for VGG19 relu4_1\n",
        "        content_loss = self.calc_content_loss(out_content_features, style_norm_content)\n",
        "        # get style features\n",
        "        out_style_features = self.encoder(out, last_only=False)   # for VGG19 [relu1_1, relu2_1, relu3_1, relu4_1]\n",
        "        style_loss = self.calc_style_loss(out_style_features, style_features)\n",
        "\n",
        "        return content_loss, style_loss\n",
        "      # End\n",
        "\n",
        "      def style_transfer(self, content, style, alpha=1.0):\n",
        "        ###########################################\n",
        "        # Encoder pass of content and style images\n",
        "        ###########################################\n",
        "        style_features = self.encoder(style, last_only=True)   # for VGG19 relu4_1\n",
        "        content_features = self.encoder(content, last_only=True)  # for VGG19 relu4_1\n",
        "        ###########################################\n",
        "        # AdaIn step\n",
        "        ###########################################\n",
        "        style_norm_content = self.adain_layer(content_features, style_features)\n",
        "        style_norm_content = alpha*style_norm_content + (1-alpha)*content_features\n",
        "        return self.decoder(style_norm_content)\n",
        "      # End\n",
        "\n",
        "#########################\n",
        "## Unit test\n",
        "#########################\n",
        "run=False\n",
        "if run==True:\n",
        "  ###############################\n",
        "  ### Get images\n",
        "  ###############################\n",
        "  print(\"==== CONTENT ====\")\n",
        "  content_iter = make_content_iter(1, 1)\n",
        "  content_images = content_iter.next()\n",
        "  print('images shape on batch size = {}'.format(content_images.size()))\n",
        "  denormed = denorm(content_images.to(device))\n",
        "  grid = torchvision.utils.make_grid(torch.cat((content_images.to(cpu), denormed.to(cpu)), dim = 0))\n",
        "  plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "  plt.show()\n",
        "  print(content_images[0])\n",
        "  print(\"==== STYLE ====\")\n",
        "  style_iter = make_style_iter(1, 1)\n",
        "  style_images = style_iter.next()\n",
        "  print('images shape on batch size = {}'.format(style_images.size()))\n",
        "  denormed = denorm(style_images.to(device))\n",
        "  grid = torchvision.utils.make_grid(torch.cat((style_images.to(cpu), denormed.to(cpu)), dim = 0))\n",
        "  plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "  plt.show()\n",
        "  print(style_images[0])\n",
        "  ###############################\n",
        "  ### Pass images through model\n",
        "  ###############################\n",
        "  model = style_transfer_net()\n",
        "  print(model)\n",
        "  model.to(device)\n",
        "  loss_c, loss_s = model(content_images.to(device), style_images.to(device))\n",
        "  print(\"loss_c={}\".format(loss_c))\n",
        "  print(\"loss_s={}\".format(loss_s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3JqMZ-MZJ7w",
        "colab_type": "text"
      },
      "source": [
        "\"We train our network using **MS-COCO [36] as content\n",
        "images** and a dataset of paintings mostly collected from\n",
        "**WikiArt [39] as style images**, following the setting of [6].\n",
        "Each dataset contains roughly 80; 000 training examples.\n",
        "We use the adam optimizer [26] and a **batch size of 8**\n",
        "content-style image pairs. During training, we **first resize\n",
        "the smallest dimension of both images to 512 while preserving the aspect ratio, then randomly crop regions of size\n",
        "256 × 256**. Since our network is fully convolutional, it can\n",
        "be applied to images of any size during testing.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoEBMl3hjJsU",
        "colab_type": "text"
      },
      "source": [
        "# LR scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sduBuDTIvgQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def make_lr_sched(opt):\n",
        "  return lr_scheduler.LambdaLR(opt, lambda iter_i: 1 / (1 + GLOBAL_ARGS['lr_decay'] * iter_i), last_epoch=-1)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "      return param_group['lr']\n",
        "# End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VD0s6BPjSP1",
        "colab_type": "text"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR7KlT-VEmOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "if GLOBAL_ARGS['MODE'] == 'train':\n",
        "  !rm -fr logs\n",
        "  !mkdir -p logs/style\n",
        "  !mkdir -p logs/content\n",
        "elif GLOBAL_ARGS['MODE'] == 'resume_train':\n",
        "  !cp -r \"/gdrive/My Drive/Colab Notebooks/adaptive_style_transfer/models/logs\" \"./\"\n",
        "\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer_style = SummaryWriter(log_dir='logs/style/')\n",
        "writer_content = SummaryWriter(log_dir='logs/content/')\n",
        "writer_lr = SummaryWriter(log_dir='logs/lr/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8onZcXNnjY0Y",
        "colab_type": "text"
      },
      "source": [
        "# Style Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZSaidmNfRLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perform_style_transfer(model, content_images, style_images, display=False):\n",
        "  # move all to device\n",
        "  for _x in [model, content_images, style_images]:\n",
        "    _x.to(device)\n",
        "  # set eval mode\n",
        "  model.eval()\n",
        "  # freeze model gradients\n",
        "  with torch.no_grad():\n",
        "    out = model.style_transfer(content_images, style_images)\n",
        "    out = denorm(out)\n",
        "  if display:\n",
        "    # display\n",
        "    all_imgs = torch.cat((content_images, style_images, out), dim=0).to(cpu)\n",
        "    grid = torchvision.utils.make_grid(all_imgs)\n",
        "    grid = grid.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(grid)\n",
        "    plt.show()\n",
        "  return out, all_imgs\n",
        "# END"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypEOKVcMfTZ1",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxSu7jci1iyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def train_iter(model, i, content_iter, style_iter, optimizer, lr_sched):\n",
        "      model.to(device)\n",
        "      model.train()\n",
        "      # pass images through network, get loss\n",
        "      content_images = next(content_iter).to(device)\n",
        "      style_images = next(style_iter).to(device)\n",
        "\n",
        "      loss_c, loss_s = model(content_images, style_images)\n",
        "      loss_c = GLOBAL_ARGS['loss_content_w'] * loss_c\n",
        "      loss_s = GLOBAL_ARGS['loss_style_w'] * loss_s\n",
        "      loss = loss_c + loss_s\n",
        "      # backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # LR scheduler\n",
        "      lr_sched.step()\n",
        "      # loss metrics\n",
        "      writer_style.add_scalar('AdaSTN/loss', loss_s, i)\n",
        "      writer_content.add_scalar('AdaSTN/loss', loss_c, i)\n",
        "      writer_lr.add_scalar('AdaSTN/LR', get_lr(optimizer), i)\n",
        "      writer_style.flush()\n",
        "      writer_content.flush()\n",
        "      writer_lr.flush()\n",
        "      # Checkpoint\n",
        "      if (i % 1600 == 0):\n",
        "        # save checkpoint\n",
        "        model_save_name = '{}/iter_{}.pth'.format(model_save_path, i)\n",
        "        tqdm.write(\"saving model checkpoint: {}\".format(model_save_name))\n",
        "        torch.save({\n",
        "              'iter': i,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'lr_scheduler_state_dict': lr_sched.state_dict(),\n",
        "              }, model_save_name)\n",
        "        # print images once in a while\n",
        "        tqdm.write(\"printing train images results: (content, style, out)\")\n",
        "        content_images = denorm(content_images)\n",
        "        style_images = denorm(style_images)\n",
        "        _ , grid = perform_style_transfer(model, content_images, style_images, display=True)\n",
        "        img_save_name = '{}/images/iter_{}.png'.format(model_save_path, i)\n",
        "        save_image(grid, img_save_name)\n",
        "\n",
        "# checkpoint : if None, start model train from scratch\n",
        "#              if not None, load existing state\n",
        "def train(model, checkpoint=None):\n",
        "  # create iters\n",
        "  content_iter = make_content_iter(batch_size=GLOBAL_ARGS['batch_size'])\n",
        "  style_iter = make_style_iter(batch_size=GLOBAL_ARGS['batch_size'])\n",
        "  # load model dict (before declaring optimizer)\n",
        "  # I think i need to do it before optimizer because when we declare optimizer\n",
        "  # we pass the model parameters\n",
        "  if checkpoint:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  # create optimizer\n",
        "  if GLOBAL_ARGS['optimizer'] == 'adam':\n",
        "    optimizer = Adam(model.parameters(), lr=GLOBAL_ARGS['lr'])\n",
        "  else:\n",
        "    optimizer = SGD(model.parameters(), lr=GLOBAL_ARGS['lr'], momentum=GLOBAL_ARGS['momentum'])\n",
        "  # make LR scheduler\n",
        "  lr_sched = make_lr_sched(optimizer)\n",
        "\n",
        "  START_ITER = 0\n",
        "  if checkpoint:\n",
        "    # when loading optimizr, needs to move it's stored tensors to gpu device\n",
        "    # a bit ugly - but there is no optimizer.to(device) code ... \n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    for state in optimizer.state.values():\n",
        "      for k, v in state.items():\n",
        "          if isinstance(v, torch.Tensor):\n",
        "              state[k] = v.to(device)\n",
        "\n",
        "    lr_sched.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
        "    START_ITER = checkpoint['iter'] + 1\n",
        "    assert (START_ITER > 0)\n",
        "\n",
        "  # because ive split the datasets in 4 - I'm dividing the loop counter by 4\n",
        "  # if training entire dataset all at once - dont divide by 4 ...\n",
        "  ITERS = range(START_ITER, START_ITER + GLOBAL_ARGS['max_iters_per_split'])\n",
        "  for i in tqdm(ITERS, initial=START_ITER):\n",
        "    train_iter(model, i, content_iter, style_iter, optimizer, lr_sched)\n",
        "  # End of for\n",
        "  model_save_name = '{}/final_iter_{}.pth'.format(model_save_path, i)\n",
        "  print(\"saving model checkpoint: {}\".format(model_save_name))\n",
        "  torch.save({\n",
        "      'iter': i,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'lr_scheduler_state_dict': lr_sched.state_dict(),\n",
        "      }, model_save_name)\n",
        "  !cp -r  \"./logs\" \"/gdrive/My Drive/Colab Notebooks/adaptive_style_transfer/models/\"\n",
        "# End"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZb0VTe265vp",
        "colab_type": "text"
      },
      "source": [
        "# Model eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3M8AUO567ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(model): \n",
        "  model.eval()\n",
        "  # create network instance\n",
        "  model = style_transfer_net()\n",
        "  # define iters\n",
        "  content_iter = make_content_iter(batch_size=GLOBAL_ARGS['batch_size'])\n",
        "  style_iter = make_style_iter(batch_size=GLOBAL_ARGS['batch_size'])\n",
        "  # get images\n",
        "  content_images = next(content_iter).to(device)\n",
        "  style_images = next(style_iter).to(device)\n",
        "  # perform style transfer\n",
        "  perform_style_transfer(model. content_images, style_images, display=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-83thI5GgXfG",
        "colab_type": "text"
      },
      "source": [
        "# train / eval / resume wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XBU50baSirwu",
        "colab": {}
      },
      "source": [
        "# create network instance\n",
        "model = style_transfer_net()\n",
        "\n",
        "if GLOBAL_ARGS['MODE'] == 'train':  \n",
        "  # train model\n",
        "  train(model)\n",
        "elif GLOBAL_ARGS['MODE'] == 'resume_train':\n",
        "  # resume training\n",
        "  saved_checkpoint_path=model_save_path + GLOBAL_ARGS['resume_train_from']\n",
        "  print(\"loading model from {}\".format(saved_checkpoint_path))\n",
        "  checkpoint = torch.load(saved_checkpoint_path)\n",
        "  train(model, checkpoint)\n",
        "elif GLOBAL_ARGS['MODE'] == 'eval':\n",
        "  saved_checkpoint_path=model_save_path + GLOBAL_ARGS['eval_model_path']\n",
        "  checkpoint = torch.load(saved_checkpoint_path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  model_eval(model)\n",
        "else:\n",
        "  assert 0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}