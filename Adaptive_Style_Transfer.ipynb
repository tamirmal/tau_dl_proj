{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adaptive Style Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamirmal/tau_dl_proj/blob/master/Adaptive_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUJvA3lpAHZQ",
        "colab_type": "code",
        "outputId": "d3bf2da2-9208-45c7-bacf-a57f172b05f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "#!pip install ipdb\n",
        "#import ipdb\n",
        "\n",
        "import datetime, os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# For tensorboard use TF 2.x+\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "# Mount GCP bucket\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'tau-dl'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!mkdir -p /content/adaptive_style_transfer/\n",
        "!gcsfuse --implicit-dirs adaptive_style_transfer /content/adaptive_style_transfer/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Updated property [core/project].\n",
            "gs://adaptive_style_transfer/\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   659  100   659    0     0  24407      0 --:--:-- --:--:-- --:--:-- 24407\n",
            "OK\n",
            "68 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 68 not upgraded.\n",
            "Need to get 4,274 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 145605 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.28.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.28.1) ...\n",
            "Setting up gcsfuse (0.28.1) ...\n",
            "Using mount point: /content/adaptive_style_transfer\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YORkr-pB-LzL",
        "colab_type": "text"
      },
      "source": [
        "![adain_net](https://i.imgur.com/jAyz9hY.jpg)\n",
        "\n",
        "Implementing https://arxiv.org/pdf/1703.06868.pdf\n",
        "There is an official reference in Torch / Lua @ https://github.com/xunhuang1995/AdaIN-style/\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "AdaIN Layer implements the following :\n",
        "\n",
        "![adain_layer](https://i.imgur.com/OiqyfkN.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYATh4Ri_DhA",
        "colab_type": "code",
        "outputId": "b489e26b-dd6f-4106-bf2d-d5b7485624b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def get_mu_and_sigma(features):\n",
        "    # input is a tensor of shape : [minibatch_size, channels, h ,w]\n",
        "    # output is a tensor of shape : [minibatch_size, channels, 1 ,1]\n",
        "\n",
        "    epsilon = 1e-6\n",
        "    minibatch_size, channels = features.size()[:2]\n",
        "\n",
        "    features_channels_stacked = features.reshape(minibatch_size, channels, -1)\n",
        "\n",
        "    features_mean_per_channel = features_channels_stacked.mean(dim=2)\n",
        "    features_mean_per_channel = features_mean_per_channel.reshape(minibatch_size, channels, 1, 1) # set dim as tensor\n",
        "\n",
        "    features_sigma_per_channel = features_channels_stacked.std(dim=2)\n",
        "    features_sigma_per_channel = features_sigma_per_channel.reshape(minibatch_size, channels, 1, 1) # set dim as tensor\n",
        "\n",
        "    return features_mean_per_channel, features_sigma_per_channel\n",
        "\n",
        "class vgg19_encoder(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(vgg19_encoder, self).__init__()\n",
        "          \n",
        "          encoder = torchvision.models.vgg19(pretrained=True, progress=True)\n",
        "          print(encoder) # print encoder, to make sure i'm extracting the correct layers\n",
        "          encoder_layers = list(encoder.features.children())\n",
        "          relu1_1 = 2\n",
        "          relu2_1 = 7\n",
        "          relu3_1 = 12\n",
        "          relu4_1 = 21\n",
        "          \n",
        "          # style encoders - we need to extract intermediate features from SEVERAL layers\n",
        "          # by splitting the model to parts we can take each part output AND feed it into next model part\n",
        "          self.encoder_1 = nn.Sequential(*encoder_layers[:relu1_1])         # input -> relu1_1\n",
        "          self.encoder_2 = nn.Sequential(*encoder_layers[relu1_1:relu2_1])  # relu1_1 -> relu2_1\n",
        "          self.encoder_3 = nn.Sequential(*encoder_layers[relu2_1:relu3_1])  # relu2_1 -> relu3_1\n",
        "          self.encoder_4 = nn.Sequential(*encoder_layers[relu3_1:relu4_1])  # relu3_1 -> relu4_1\n",
        "\n",
        "          # Encoder IS NOT trainable - freeze it\n",
        "          for e in [self.encoder_1, self.encoder_2, self.encoder_3, self.encoder_4]:\n",
        "              for p in e.parameters():\n",
        "                  p.requires_grad = False\n",
        "        # END of __init__()\n",
        "\n",
        "      def forward(self, x, last_only = True):\n",
        "        #\n",
        "        #  ENC1 --- ENC2 --- ENC3 --- ENC4 ---\n",
        "        #        |        |        |        |\n",
        "        #     relu1_1    relu2_1  relu3_1  relu4_1\n",
        "\n",
        "        # last_only : pass only the output of relu4_1 layer\n",
        "        features = [ x ]\n",
        "\n",
        "        features_1 = self.encoder_1(x)\n",
        "        features_2 = self.encoder_2(features_1)\n",
        "        features_3 = self.encoder_3(features_2)\n",
        "        features_4 = self.encoder_4(features_3)\n",
        "        \n",
        "        if last_only is True:\n",
        "          return features_4\n",
        "        else:\n",
        "          features.append(features_1)\n",
        "          features.append(features_2)\n",
        "          features.append(features_3)\n",
        "          features.append(features_4)\n",
        "          features = features[1:]\n",
        "          return features\n",
        "      # END of forward()\n",
        "\n",
        "\n",
        "vgg19_decoder = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "          )\n",
        "\n",
        "class style_transfer_net(nn.Module):\n",
        "      def __init__(self):\n",
        "        super(style_transfer_net, self).__init__()\n",
        "\n",
        "        ## using VGG as encoder/decoder\n",
        "        ## TODO : consider using other architectures as suggested in the article\n",
        "        ##        such as resnet34 etc. which are deep BUT have good convergence due to skip-connection (residuals)\n",
        "        \n",
        "        encoder_t = 'VGG19' # TODO in the future this will be external argument\n",
        "        if encoder_t == 'VGG19':\n",
        "          self.encoder = vgg19_encoder()\n",
        "\n",
        "        decoder_t = 'VGG19'\n",
        "        if decoder_t == 'VGG19':\n",
        "          self.decoder = vgg19_decoder\n",
        "      # End\n",
        "\n",
        "      @staticmethod\n",
        "      def adain_layer(content_features, style_features):\n",
        "          # Adaptive instance normalization\n",
        "          # Inputs are :\n",
        "          #  content features - the content image output from VGG_ENCODER.relu4_1     [batch_size, 512, h/8, w/8]\n",
        "          #  style features   - the style image output from VGG_ENCODER.relu4_1       [batch_size, 512, h/8, w/8]\n",
        "          # Therefore they have the same dimensions of 512x(H/8)x(W/8)\n",
        "          # This layer calculates a per-channel mean and std of the style features\n",
        "          # and scales the content features so they have the same mean and std (per channel) of the style\n",
        "          \n",
        "          content_mu, content_sigma = get_mu_and_sigma(content_features)\n",
        "          style_mu, style_sigma = get_mu_and_sigma(style_features)\n",
        "\n",
        "          normalized_content_features = (content_features - content_mu) / content_sigma\n",
        "          style_normalized_content_features = style_sigma*normalized_content_features + style_mu\n",
        "          return style_normalized_content_features\n",
        "\n",
        "\n",
        "      @staticmethod\n",
        "      def calc_content_loss(out_content, adain_content):\n",
        "          return F.mse_loss(out_content, adain_content)\n",
        "\n",
        "      @staticmethod\n",
        "      def calc_style_loss(out_style, in_style):\n",
        "          loss = 0\n",
        "          for a,b in zip(out_style, in_style):\n",
        "              a_mu, a_sigma = get_mu_and_sigma(a)\n",
        "              b_mu, b_sigma = get_mu_and_sigma(b)\n",
        "              loss += F.mse_loss(a_mu, b_mu) + F.mse_loss(a_sigma, b_sigma)\n",
        "          return loss\n",
        "\n",
        "      def forward(self, content, style, alpha=1.0):\n",
        "        assert alpha >= 0\n",
        "        assert alpha <= 1\n",
        "        # TODO - add asserts that encoders are NOT trainable !!!\n",
        "\n",
        "        ###########################################\n",
        "        # Encoder pass of content and style images\n",
        "        ###########################################\n",
        "        style_features = self.encoder(style, last_only=False)   # for VGG19 [relu1_1, relu2_1, relu3_1, relu4_1]\n",
        "        content_features = self.encoder(style, last_only=True)  # for VGG19 relu4_1\n",
        "\n",
        "        ###########################################\n",
        "        # AdaIn step\n",
        "        ###########################################\n",
        "        # feed into AdaIn layer the style & content features, get style-normalized content features\n",
        "        style4=style_features[-1]\n",
        "        style_norm_content = self.adain_layer(content_features, style4)\n",
        "        style_norm_content = alpha*style_norm_content + (1-alpha)*content_features # hyper-parameter, a tradeoff between content and style\n",
        "        \n",
        "        ###########################################\n",
        "        # Apply the style transfer\n",
        "        ###########################################\n",
        "        # pass through decoder, obtain transformed image\n",
        "        out = self.decoder(style_norm_content)\n",
        "\n",
        "        ###########################################\n",
        "        # Loss calculation\n",
        "        ###########################################\n",
        "        # get content & style features of output image (after style transfer), same process as above\n",
        "        # TODO - enclose this in a function, too much repeating code\n",
        "        out_content_features = self.encoder(out, last_only=True)  # for VGG19 relu4_1\n",
        "        content_loss = self.calc_content_loss(out_content_features, style_norm_content)\n",
        "        # get style features\n",
        "        out_style_features = self.encoder(out, last_only=False)   # for VGG19 [relu1_1, relu2_1, relu3_1, relu4_1]\n",
        "        style_loss = self.calc_style_loss(out_style_features, style_features)\n",
        "        # combine the losses\n",
        "        return content_loss, style_loss\n",
        "      # End\n",
        "\n",
        "model = style_transfer_net()\n",
        "print(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace=True)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace=True)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace=True)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "style_transfer_net(\n",
            "  (encoder): vgg19_encoder(\n",
            "    (encoder_1): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (encoder_2): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): ReLU(inplace=True)\n",
            "    )\n",
            "    (encoder_3): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): ReLU(inplace=True)\n",
            "    )\n",
            "    (encoder_4): Sequential(\n",
            "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): Upsample(scale_factor=2.0, mode=nearest)\n",
            "    (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgAqs_HVkGCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "089591ba-582f-43c1-d232-08bdd2115055"
      },
      "source": [
        "from torch.utils import data\n",
        "from PIL import Image, ImageFile\n",
        "\n",
        "class DatasetFolders(data.Dataset):\n",
        "    def __init__(self, files_paths, transform):\n",
        "        super(DatasetFolders, self).__init__()\n",
        "        with open(files_paths) as F:\n",
        "          paths = F.readlines()\n",
        "        self.paths = [x.strip() for x in paths] \n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        path = path.replace('gs://', '/content/')\n",
        "        img = Image.open(str(path)).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def name(self):\n",
        "        return 'DatasetFolders'\n",
        "\n",
        "\n",
        "trans = [\n",
        "    transforms.Resize(size=(512, 512)),\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "]\n",
        "trans = transforms.Compose(trans)\n",
        "\n",
        "##### Ive manually calculated the dataset length ... lazy to write something better\n",
        "## Number of content examples:\n",
        "## 82783 /root/content_examples_paths.lst\n",
        "## Number of style examples:\n",
        "## 79433 /root/style_examples_paths.lst\n",
        "\n",
        "#!gsutil ls gs://adaptive_style_transfer/COCO_SPLITS/*/*.jpg > /content/adaptive_style_transfer/content_examples_paths.lst\n",
        "!gsutil cp gs://adaptive_style_transfer/content_examples_paths.lst /root/\n",
        "!echo \"Number of content examples:\"\n",
        "!wc -l /root/content_examples_paths.lst\n",
        "content_dataset = DatasetFolders('/root/content_examples_paths.lst', trans)\n",
        "\n",
        "#!gsutil ls gs://adaptive_style_transfer/WIKIART_SPLITS/*/*.jpg > /content/adaptive_style_transfer/style_examples_paths.lst\n",
        "!gsutil cp gs://adaptive_style_transfer/style_examples_paths.lst /root/\n",
        "!echo \"Number of style examples:\"\n",
        "!wc -l /root/style_examples_paths.lst\n",
        "style_dataset = DatasetFolders('/root/style_examples_paths.lst', trans)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://adaptive_style_transfer/content_examples_paths.lst...\n",
            "/ [1 files][  7.0 MiB/  7.0 MiB]                                                \n",
            "Operation completed over 1 objects/7.0 MiB.                                      \n",
            "Number of content examples:\n",
            "82783 /root/content_examples_paths.lst\n",
            "Copying gs://adaptive_style_transfer/style_examples_paths.lst...\n",
            "/ [1 files][  4.7 MiB/  4.7 MiB]                                                \n",
            "Operation completed over 1 objects/4.7 MiB.                                      \n",
            "Number of style examples:\n",
            "79433 /root/style_examples_paths.lst\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3JqMZ-MZJ7w",
        "colab_type": "text"
      },
      "source": [
        "\"We train our network using **MS-COCO [36] as content\n",
        "images** and a dataset of paintings mostly collected from\n",
        "**WikiArt [39] as style images**, following the setting of [6].\n",
        "Each dataset contains roughly 80; 000 training examples.\n",
        "We use the adam optimizer [26] and a **batch size of 8**\n",
        "content-style image pairs. During training, we **first resize\n",
        "the smallest dimension of both images to 512 while preserving the aspect ratio, then randomly crop regions of size\n",
        "256 Ã— 256**. Since our network is fully convolutional, it can\n",
        "be applied to images of any size during testing.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp8FnasfYpN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sd = 999\n",
        "def torch_seed():\n",
        "    global sd\n",
        "    torch.manual_seed(sd)\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(sd)\n",
        "\n",
        "## Training will not be in epochs but in iterations\n",
        "## the dataloaders will need to be infinite\n",
        "## (in epoch we finish an epoch at end of iteration)\n",
        "## (doing as in the article/lua-torch code. training for 160,000 iteration with batch size of 8)\n",
        "\n",
        "# the built in samplers in pytorch are not infinite, defining my own\n",
        "def InfiniteSampler(dataset_len):\n",
        "    torch_seed()\n",
        "    perm_iter = iter(torch.randperm(dataset_len).tolist())\n",
        "    while True:\n",
        "      try:\n",
        "        yield perm_iter.__next__()\n",
        "      except StopIteration:\n",
        "        perm_iter = iter(torch.randperm(dataset_len).tolist())\n",
        "        yield perm_iter.__next__()\n",
        "\n",
        "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
        "    def __init__(self, data_source):\n",
        "        self.num_samples = len(data_source)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(InfiniteSampler(self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return 2 ** 31\n",
        "\n",
        "content_iter = iter(data.DataLoader(\n",
        "    content_dataset, batch_size=GLOBAL_ARGS['batch_size'],\n",
        "    sampler=InfiniteSamplerWrapper(content_dataset),\n",
        "    num_workers=4))\n",
        "style_iter = iter(data.DataLoader(\n",
        "    style_dataset, batch_size=GLOBAL_ARGS['batch_size'],\n",
        "    sampler=InfiniteSamplerWrapper(style_dataset),\n",
        "    num_workers=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw6q_UF5be6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "content_iter = iter(data.DataLoader(\n",
        "    content_dataset, batch_size=1,\n",
        "    sampler=InfiniteSamplerWrapper(content_dataset),\n",
        "    num_workers=1))\n",
        "style_iter = iter(data.DataLoader(\n",
        "    style_dataset, batch_size=1,\n",
        "    sampler=InfiniteSamplerWrapper(style_dataset),\n",
        "    num_workers=1))\n",
        "\n",
        "images = content_iter.next()\n",
        "print('images shape on batch size = {}'.format(images.size()))\n",
        "grid = torchvision.utils.make_grid(images)\n",
        "plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "print(images[0])\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sduBuDTIvgQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "\n",
        "# as in original LUA implemantation\n",
        "def adjust_lr(optimizer, iter_i):\n",
        "    lr = GLOBAL_ARGS['lr'] / (1.0 + GLOBAL_ARGS['lr_decay'] * iter_i)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "if GLOBAL_ARGS['optimizer'] == 'adam':\n",
        "  optimizer = Adam(model.parameters(), lr=GLOBAL_ARGS['lr'])\n",
        "else:\n",
        "  optimizer = SGD(model.parameters(), lr=GLOBAL_ARGS['lr'], momentum=GLOBAL_ARGS['momentum'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxSu7jci1iyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d958b722-c028-4c52-fc8e-def9ecae7dd2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  # no GPU available\n",
        "  assert 0\n",
        "\n",
        "## Train the network\n",
        "for i in tqdm(range(GLOBAL_ARGS['max_iters'])):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    adjust_lr(optimizer, iter_i=i)\n",
        "    content_images = next(content_iter).to(device)\n",
        "    style_images = next(style_iter).to(device)\n",
        "    loss_c, loss_s = model(content_images, style_images)\n",
        "    loss_c = GLOBAL_ARGS['loss_content_w'] * loss_c\n",
        "    loss_s = GLOBAL_ARGS['loss_style_w'] * loss_s\n",
        "    loss = loss_c + loss_s\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "# end of for"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 108/160000 [27:11<593:09:19, 13.36s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uExNzZyyRLM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d1fa4a95-fcd6-46a8-9e1e-53c111c342ed"
      },
      "source": [
        "GLOBAL_ARGS = {}\n",
        "GLOBAL_ARGS['lr'] = 1e-4\n",
        "GLOBAL_ARGS['lr_decay'] = 5e-5\n",
        "GLOBAL_ARGS['momentum'] = 0.9\n",
        "GLOBAL_ARGS['optimizer'] = 'adam'\n",
        "GLOBAL_ARGS['batch_size'] = 8\n",
        "GLOBAL_ARGS['max_iters'] = 160000\n",
        "GLOBAL_ARGS['loss_style_w'] = 1e-2\n",
        "GLOBAL_ARGS['loss_content_w'] = 1\n",
        "\n",
        "\"\"\"\n",
        "-- Training options\n",
        "cmd:option('-resume', false, 'If true, resume training from the last checkpoint')\n",
        "cmd:option('-optimizer', 'adam', 'Optimizer used, adam|sgd')\n",
        "cmd:option('-learningRate', 1e-4, 'Learning rate')\n",
        "cmd:option('-learningRateDecay', 5e-5, 'Learning rate decay')\n",
        "cmd:option('-momentum', 0.9, 'Momentum')\n",
        "cmd:option('-weightDecay', 0, 'Weight decay')\n",
        "cmd:option('-batchSize', 8, 'Batch size')\n",
        "cmd:option('-maxIter', 160000, 'Maximum number of iterations')\n",
        "cmd:option('-targetContentLayer', 'relu4_1', 'Target content layer used to compute the loss')\n",
        "cmd:option('-targetStyleLayers', 'relu1_1,relu2_1,relu3_1,relu4_1', 'Target style layers used to compute the loss')\n",
        "cmd:option('-tvWeight', 0, 'Weight of TV loss')\n",
        "cmd:option('-styleWeight', 1e-2, 'Weight of style loss')\n",
        "cmd:option('-contentWeight', 1, 'Weight of content loss')\n",
        "cmd:option('-reconStyle', false, 'If true, the decoder is also trained to reconstruct style images')\n",
        "cmd:option('-normalize', false, 'If true, gradients at the loss function are normalized')\n",
        "\"\"\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n-- Training options\\ncmd:option('-resume', false, 'If true, resume training from the last checkpoint')\\ncmd:option('-optimizer', 'adam', 'Optimizer used, adam|sgd')\\ncmd:option('-learningRate', 1e-4, 'Learning rate')\\ncmd:option('-learningRateDecay', 5e-5, 'Learning rate decay')\\ncmd:option('-momentum', 0.9, 'Momentum')\\ncmd:option('-weightDecay', 0, 'Weight decay')\\ncmd:option('-batchSize', 8, 'Batch size')\\ncmd:option('-maxIter', 160000, 'Maximum number of iterations')\\ncmd:option('-targetContentLayer', 'relu4_1', 'Target content layer used to compute the loss')\\ncmd:option('-targetStyleLayers', 'relu1_1,relu2_1,relu3_1,relu4_1', 'Target style layers used to compute the loss')\\ncmd:option('-tvWeight', 0, 'Weight of TV loss')\\ncmd:option('-styleWeight', 1e-2, 'Weight of style loss')\\ncmd:option('-contentWeight', 1, 'Weight of content loss')\\ncmd:option('-reconStyle', false, 'If true, the decoder is also trained to reconstruct style images')\\ncmd:option('-normalize', false, 'If true, gradients at the loss function are normalized')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}